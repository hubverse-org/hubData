% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/connect_target_oracle.R
\name{connect_target_oracle_output}
\alias{connect_target_oracle_output}
\title{Open connection to oracle-output target data}
\usage{
connect_target_oracle_output(
  hub_path = ".",
  na = c("NA", ""),
  ignore_files = NULL,
  output_type_id_datatype = c("from_config", "auto", "character", "double", "integer",
    "logical", "Date")
)
}
\arguments{
\item{hub_path}{Either a character string path to a local Modeling Hub directory
or an object of class \verb{<SubTreeFileSystem>} created using functions \code{\link[=s3_bucket]{s3_bucket()}}
or \code{\link[=gs_bucket]{gs_bucket()}} by providing a string S3 or GCS bucket name or path to a
Modeling Hub directory stored in the cloud.
For more details consult the
\href{https://arrow.apache.org/docs/r/articles/fs.html}{Using cloud storage (S3, GCS)}
in the \code{arrow} package.
The hub must be fully configured with valid \code{admin.json} and \code{tasks.json}
files within the \code{hub-config} directory.}

\item{na}{A character vector of strings to interpret as missing values. Only
applies to CSV files. The default is \code{c("NA", "")}. Useful when actual character
string \code{"NA"} values are used in the data. In such a case, use empty cells to
indicate missing values in your files and set \code{na = ""}.}

\item{ignore_files}{A character vector of file \strong{names} (not paths) or
file \strong{prefixes} to ignore when discovering model output files to
include in dataset connections.
Parent directory names should not be included.
Common non-data files such as \code{"README"} and \code{".DS_Store"} are ignored automatically,
but additional files can be excluded by specifying them here.}

\item{output_type_id_datatype}{character string. One of \code{"from_config"}, \code{"auto"},
\code{"character"}, \code{"double"}, \code{"integer"}, \code{"logical"}, \code{"Date"}.
Defaults to \code{"from_config"} which uses the setting in the \code{output_type_id_datatype}
property in the \code{tasks.json} config file if available. If the property is
not set in the config, the argument falls back to \code{"auto"} which determines
the  \code{output_type_id} data type automatically from the \code{tasks.json}
config file as the simplest data type required to represent all output
type ID values across all output types in the hub.
When only point estimate output types (where \code{output_type_id}s are \code{NA},) are
being collected by a hub, the \code{output_type_id} column is assigned a \code{character}
data type when auto-determined.
Other data type values can be used to override automatic determination.
Note that attempting to coerce \code{output_type_id} to a data type that is
not valid for the data (e.g. trying to coerce\code{"character"} values to
\code{"double"}) will likely result in an error or potentially unexpected
behaviour so use with care.}
}
\value{
An arrow dataset object of subclass <target_oracle_output>.
}
\description{
\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#experimental}{\figure{lifecycle-experimental.svg}{options: alt='[Experimental]'}}}{\strong{[Experimental]}} Open the oracle-output target data file(s)
in a hub as an arrow dataset.
}
\details{
If the target data is split across multiple files in a \code{oracle-output} directory,
all files must share the same file format, either csv or parquet.
No other types of files are currently allowed in a \code{oracle-output} directory.
\subsection{Schema Creation}{

This function uses different methods to create the Arrow schema depending on
the hub configuration version:

\strong{v6+ hubs (with \code{target-data.json}):} Schema is created directly from the
\code{target-data.json} configuration file using \code{\link[=create_oracle_output_schema]{create_oracle_output_schema()}}.
This config-based approach is fast and deterministic, requiring no filesystem
I/O to scan data files. It's especially beneficial for cloud storage where
file scanning can be slow.

\strong{Pre-v6 hubs (without \code{target-data.json}):} Schema is inferred by scanning
the actual data files. This inference-based approach examines file structure
and content to determine column types.

The function automatically detects which method to use based on the presence
of \code{target-data.json} in the hub configuration.
}

\subsection{Schema Ordering}{

Column ordering in the resulting dataset depends on configuration version and file format:

\strong{v6+ hubs (with \code{target-data.json}):}
\itemize{
\item \strong{Parquet}: Columns are reordered to the standard hubverse convention (see \code{\link[=get_target_data_colnames]{get_target_data_colnames()}}).
Parquet's column-by-name matching enables safe reordering.
\item \strong{CSV}: Original file ordering is preserved to avoid column name/position mismatches during collection.
}

\strong{Pre-v6 hubs}: Original file ordering is preserved regardless of format.
}
}
\examples{
# Column Ordering: CSV vs Parquet in v6+ hubs
# For v6+ hubs with target-data.json, ordering differs by file format

# Example 1: CSV format (single file) - preserves original file ordering
hub_path_csv <- system.file("testhubs/v6/target_file", package = "hubUtils")
oo_con_csv <- connect_target_oracle_output(hub_path_csv)

# CSV columns are in their original file order
names(oo_con_csv)

# Collect and filter as usual
oo_con_csv |> dplyr::collect()
oo_con_csv |>
  dplyr::filter(location == "US") |>
  dplyr::collect()

# Example 2: Parquet format (directory) - reordered to hubverse convention
hub_path_parquet <- system.file("testhubs/v6/target_dir", package = "hubUtils")
oo_con_parquet <- connect_target_oracle_output(hub_path_parquet)

# Parquet columns follow hubverse convention (date first, then alphabetical)
names(oo_con_parquet)

# Reordering is safe for Parquet because it matches columns by name
# rather than position during collection
oo_con_parquet |> dplyr::collect()

# Both formats support the same filtering operations
oo_con_parquet |>
  dplyr::filter(target_end_date ==  "2022-12-31") |>
  dplyr::collect()

# Get distinct target_end_date values
oo_con_parquet |>
  dplyr::distinct(target_end_date) |>
  dplyr::pull(as_vector = TRUE)

\dontrun{
# Access Target oracle-output data from a cloud hub
s3_hub_path <- s3_bucket("example-complex-forecast-hub")
s3_con <- connect_target_oracle_output(s3_hub_path)
s3_con
s3_con |> dplyr::collect()
}
}
