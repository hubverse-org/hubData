[{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying standards acceptable behavior. Enforcement responsibility Code Conduct Committee, take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Instances abusive, harassing, otherwise unacceptable behavior may reported member Code Conduct Committee. complaints reviewed investigated promptly fairly. Code Conduct Committee use Enforcement Manual determining consequences action deem violation Code Conduct. community leaders Code Conduct Committee members obligated respect privacy security reporter incident.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired Mozilla’s code conduct enforcement ladder. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to hubData","title":"Contributing to hubData","text":"outlines propose change hubData. general info contributing , hubverse packages, please see hubverse community page. can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to hubData","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). procedures contributing bigger changes, code particular, generally follow advised tidyverse dev team, including following tidyverse style guide code recording user facing changes NEWS.md.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to hubData","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"hubverse-org/hubData\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Follow pull request checklist create Git branch pull request (PR). recommend using usethis::pr_init(\"name/brief-description/issue\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first heading—usually labelled “development version”). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to hubData","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to hubData","text":"Please note hubData project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Consortium Infectious Disease Modeling Hubs Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"structure-of-hubverse-datasets","dir":"Articles","previous_headings":"","what":"Structure of hubverse datasets","title":"Accessing Model Output Data","text":"data returned connecting querying hubs can read validated model_out_tbl foundational S3 class hubverse ecosystem. model_out_tbl long-form tibble designed conform hubverse data specifications model output data. short, columns valid model_out_tbl containing model output data hub : model_id: unique character identifier model. output_type: character variable defines type representation model output given row. output_type_id: variable specifies additional identifying information specific output type given row, e.g., numeric quantile level, string giving name possible category discrete outcome, index sample. value: numeric variable provides information model’s prediction. ... : columns present depending modeling tasks defined individual modeling hub. columns referred hubverse terminology task-ID variables. hubverse tools, designed data validation, ensemble building, visualization, etc…, designed “promises” implicit data format specified model_out_tbl. example, hubEnsembles::linear_pool() function accepts input returns output model_out_tbl objects.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"hub-connections","dir":"Articles","previous_headings":"","what":"Hub connections","title":"Accessing Model Output Data","text":"two functions connecting model-output data: connect_hub() used connecting fully configured hubs (.e. contain valid admin.json tasks.json hub-config directory). function uses configurations defined config files hub-config/ directory allows connecting hubs files multiple file formats (allowable formats specified file_format property admin.json). connect_model_output() allows connecting directly contents model-output directory useful connecting appropriately organised files informal hub (.e. fully configured appropriate hub-config/ files.) functions establish connections arrow package, specifically opening datasets FileSystemDatasets, one file format. functions also able connect files stored locally cloud (e.g. AWS S3 buckets). multiple file formats accepted single Hub, file format specific FileSystemDatasets combined single UnionDataset single point access entire Hub model-output dataset. applies connect_hub() fully configured Hubs, config files can used determine unifying schema across file formats. contrast, connect_model_output() can used open single file format datasets format defined explicitly file_format argument.","code":"library(hubData) library(dplyr) #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union"},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"connecting-to-a-local-hub","dir":"Articles","previous_headings":"Connecting to a configured hub","what":"Connecting to a local hub","title":"Accessing Model Output Data","text":"connect local hub, supply path hub connect_hub()","code":"hub_path <- system.file(\"testhubs/flusight\", package = \"hubUtils\") hub_con <- hubData::connect_hub(hub_path) hub_con #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"US CDC FluSight\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/flusight #> • file_format: \"csv(5/5)\", \"parquet(2/2)\", and \"arrow(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/flusight/forecasts\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema #> hub_connection #> 8 columns #> forecast_date: date32[day] #> horizon: int32 #> target: string #> location: string #> output_type: string #> output_type_id: string #> value: double #> model_id: string"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"connecting-to-a-hub-in-the-cloud","dir":"Articles","previous_headings":"Connecting to a configured hub","what":"Connecting to a hub in the cloud","title":"Accessing Model Output Data","text":"connect hub cloud, first use one re-exported arrow helpers s3_bucket() gs_bucket() depending cloud storage provider, string bucket name/path create appropriate cloud *FileSystem object (details consult arrow article Using cloud storage (S3, GCS)). supply resulting *FileSystem object connect_hub().","code":"hub_path_cloud <- hubData::s3_bucket(\"hubverse/hubutils/testhubs/simple/\") hub_con_cloud <- hubData::connect_hub(hub_path_cloud) #> ℹ Updating superseded URL `Infectious-Disease-Modeling-hubs` to `hubverse-org` #> ℹ Updating superseded URL `Infectious-Disease-Modeling-hubs` to `hubverse-org` hub_con_cloud #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: hubverse/hubutils/testhubs/simple/ #> • file_format: \"csv(3/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"S3FileSystem\" #> • model_output_dir: \"model-output/\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string #> age_group: string"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"performance-considerations","dir":"Articles","previous_headings":"Connecting to a configured hub > Connecting to a hub in the cloud","what":"Performance considerations","title":"Accessing Model Output Data","text":"default, connect_hub() skips file validation checks creating connection, providing optimal performance especially large cloud-based hubs minimizing /O operations. working model output directory may contain files opened part dataset (e.g., non-model output files incompatible formats), can use skip_checks = FALSE connect_hub() attempt detect exclude invalid files connecting. Note negatively impact performance due increased /O operations. hubs validated hubValidations package, default skip_checks = TRUE behavior recommended. invalid files present, consider using ignore_files argument instead.","code":"hub_path_cloud <- hubData::s3_bucket(\"hubverse/hubutils/testhubs/parquet/\") hub_con_cloud <- hubData::connect_hub(hub_path_cloud, file_format = \"parquet\") #> ℹ Updating superseded URL `Infectious-Disease-Modeling-hubs` to `hubverse-org` #> ℹ Updating superseded URL `Infectious-Disease-Modeling-hubs` to `hubverse-org` hub_con_cloud #>  #> ── <hub_connection/FileSystemDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: hubverse/hubutils/testhubs/parquet/ #> • file_format: \"parquet(4/4)\" #> • checks: FALSE #> • file_system: \"S3FileSystem\" #> • model_output_dir: \"model-output/\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema #> hub_connection with 4 Parquet files #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> age_group: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"accessing-data","dir":"Articles","previous_headings":"","what":"Accessing data","title":"Accessing Model Output Data","text":"access data hub connection can use dplyr verbs construct querying pipelines. perform queries, can use dplyr’s collect() function: Note however example, output contains required model_id, output_type, output_type_id value columns model_out_tbl object, returned tbl_df tibble object order columns standardised.","code":"hub_con %>%   dplyr::filter(output_type == \"quantile\", location == \"US\") %>%   dplyr::collect() #> # A tibble: 276 × 8 #>    forecast_date horizon target        location output_type output_type_id value #>    <date>          <int> <chr>         <chr>    <chr>       <chr>          <dbl> #>  1 2023-04-24          1 wk ahead inc… US       quantile    0.01               0 #>  2 2023-04-24          1 wk ahead inc… US       quantile    0.025              0 #>  3 2023-04-24          1 wk ahead inc… US       quantile    0.05               0 #>  4 2023-04-24          1 wk ahead inc… US       quantile    0.1              281 #>  5 2023-04-24          1 wk ahead inc… US       quantile    0.15             600 #>  6 2023-04-24          1 wk ahead inc… US       quantile    0.2              717 #>  7 2023-04-24          1 wk ahead inc… US       quantile    0.25             817 #>  8 2023-04-24          1 wk ahead inc… US       quantile    0.3              877 #>  9 2023-04-24          1 wk ahead inc… US       quantile    0.35             913 #> 10 2023-04-24          1 wk ahead inc… US       quantile    0.4              965 #> # ℹ 266 more rows #> # ℹ 1 more variable: model_id <chr>"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"use-collect_hub-to-return-model_out_tbls","dir":"Articles","previous_headings":"Accessing data","what":"Use collect_hub() to return model_out_tbls","title":"Accessing Model Output Data","text":"Conveniently, can use hubData wrapper collect_hub() converts output dplyr::collect() model_out_tbl class object possible:","code":"tbl <- hub_con %>%   dplyr::filter(output_type == \"quantile\", location == \"US\") %>%   hubData::collect_hub()  tbl #> # A tibble: 276 × 8 #>    model_id     forecast_date horizon target location output_type output_type_id #>  * <chr>        <date>          <int> <chr>  <chr>    <chr>       <chr>          #>  1 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.01           #>  2 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.025          #>  3 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.05           #>  4 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.1            #>  5 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.15           #>  6 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.2            #>  7 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.25           #>  8 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.3            #>  9 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.35           #> 10 hub-baseline 2023-05-01          1 wk ah… US       quantile    0.4            #> # ℹ 266 more rows #> # ℹ 1 more variable: value <dbl>  class(tbl) #> [1] \"model_out_tbl\" \"tbl_df\"        \"tbl\"           \"data.frame\""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"accessing-data-from-cloud-hubs","dir":"Articles","previous_headings":"Accessing data","what":"Accessing data from cloud hubs","title":"Accessing Model Output Data","text":"Accessing data hubs cloud exactly :","code":"hub_con_cloud %>%   dplyr::filter(output_type == \"quantile\", location == \"US\") %>%   hubData::collect_hub() #> # A tibble: 230 × 9 #>    model_id     origin_date target        horizon location age_group output_type #>  * <chr>        <date>      <chr>           <int> <chr>    <chr>     <chr>       #>  1 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  2 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  3 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  4 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  5 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  6 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  7 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  8 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  9 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #> 10 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #> # ℹ 220 more rows #> # ℹ 2 more variables: output_type_id <dbl>, value <int>"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"limitations-of-dplyr-queries-on-arrow-datasets","dir":"Articles","previous_headings":"Accessing data","what":"Limitations of dplyr queries on arrow datasets","title":"Accessing Model Output Data","text":"Note dplyr filtering options available arrow datasets. example, wanted get quantile predictions last forecast date hub, might try: doesn’t work however arrow equivalent max method Date[32] data types. situation, collect applying first filtering level work arrow finish filtering -memory data returned collect. Alternatively, depending size data, might quicker filter data two steps: get last forecast date available filtered subset. use last forecast date filtering query.","code":"hub_con %>%   dplyr::filter(     output_type == \"quantile\",     location == \"US\",     forecast_date == max(forecast_date, na.rm = TRUE)   ) %>%   hubData::collect_hub() #> Error in `forecast_date == max(forecast_date, na.rm = TRUE)`: #> ! Expression not supported in filter() in Arrow #> → Call collect() first to pull data into R. hub_con %>%   dplyr::filter(output_type == \"quantile\", location == \"US\") %>%   hubData::collect_hub() %>%   dplyr::filter(forecast_date == max(forecast_date)) #> # A tibble: 92 × 8 #>    model_id     forecast_date horizon target location output_type output_type_id #>    <chr>        <date>          <int> <chr>  <chr>    <chr>       <chr>          #>  1 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.01           #>  2 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.025          #>  3 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.05           #>  4 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.1            #>  5 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.15           #>  6 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.2            #>  7 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.25           #>  8 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.3            #>  9 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.35           #> 10 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.4            #> # ℹ 82 more rows #> # ℹ 1 more variable: value <dbl> last_forecast <- hub_con %>%   dplyr::filter(output_type == \"quantile\", location == \"US\") %>%   dplyr::pull(forecast_date, as_vector = TRUE) %>%   max(na.rm = TRUE)   hub_con %>%   dplyr::filter(     output_type == \"quantile\",     location == \"US\",     forecast_date == last_forecast   ) %>%   hubData::collect_hub() #> # A tibble: 92 × 8 #>    model_id     forecast_date horizon target location output_type output_type_id #>  * <chr>        <date>          <int> <chr>  <chr>    <chr>       <chr>          #>  1 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.01           #>  2 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.025          #>  3 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.05           #>  4 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.1            #>  5 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.15           #>  6 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.2            #>  7 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.25           #>  8 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.3            #>  9 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.35           #> 10 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.4            #> # ℹ 82 more rows #> # ℹ 1 more variable: value <dbl>"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"use-arrowto_duckdb-to-extend-available-queries","dir":"Articles","previous_headings":"Accessing data > Limitations of dplyr queries on arrow datasets","what":"Use arrow::to_duckdb() to extend available queries","title":"Accessing Model Output Data","text":"alternatively use arrow::to_duckdb() first convert dataset connection memory virtual DuckDB table. allows run queries supported DuckDB arrow, extending potential queries can run hub data collecting. details see DuckDB quacks Arrow: zero-copy data integration Apache Arrow DuckDB.","code":"hub_con %>%   arrow::to_duckdb() %>%   dplyr::filter(     output_type == \"quantile\",     location == \"US\",     forecast_date == max(forecast_date, na.rm = TRUE)   ) %>%   hubData::collect_hub() #> # A tibble: 92 × 8 #>    model_id     forecast_date horizon target location output_type output_type_id #>  * <chr>        <date>          <int> <chr>  <chr>    <chr>       <chr>          #>  1 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.01           #>  2 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.025          #>  3 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.05           #>  4 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.1            #>  5 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.15           #>  6 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.2            #>  7 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.25           #>  8 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.3            #>  9 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.35           #> 10 hub-baseline 2023-05-08          1 wk ah… US       quantile    0.4            #> # ℹ 82 more rows #> # ℹ 1 more variable: value <dbl>"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"connecting-to-a-model-output-directory","dir":"Articles","previous_headings":"","what":"Connecting to a model output directory","title":"Accessing Model Output Data","text":"also option connect directly model output directory without using metadata hub config file. can useful hub fully configured yet. approach certain limitations though. example, overall unifying schema determined config files ability open_dataset() connect parse data correctly guaranteed across files. addition, single file_format dataset can opened. Accessing data follows procedure described fully configured hubs: connecting cloud model output data follows procedure described fully configured cloud hubs: connect_model_output() also skip_checks argument. Unlike connect_hub(), defaults skip_checks = TRUE optimal performance validated hubs, connect_model_output() defaults skip_checks = FALSE since designed work model output directories may draft form contain invalid files. can improve performance setting skip_checks = TRUE know directory contains valid model output files:","code":"model_output_dir <- system.file(   \"testhubs/simple/model-output\",   package = \"hubUtils\" ) mod_out_con <- hubData::connect_model_output(   model_output_dir,   file_format = \"csv\" ) mod_out_con #>  #> ── <mod_out_connection/FileSystemDataset> ── #>  #> • file_format: \"csv(3/3)\" #> • checks: TRUE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #>  #> ── Connection schema #> mod_out_connection with 3 csv files #> 8 columns #> origin_date: date32[day] #> target: string #> horizon: int64 #> location: string #> output_type: string #> output_type_id: double #> value: int64 #> model_id: string mod_out_con %>%   dplyr::filter(output_type == \"quantile\", location == \"US\") %>%   hubData::collect_hub() #> # A tibble: 138 × 8 #>    model_id origin_date target horizon location output_type output_type_id value #>  * <chr>    <date>      <chr>    <int> <chr>    <chr>                <dbl> <int> #>  1 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.01    135 #>  2 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.025   137 #>  3 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.05    139 #>  4 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.1     140 #>  5 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.15    141 #>  6 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.2     141 #>  7 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.25    142 #>  8 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.3     143 #>  9 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.35    144 #> 10 hub-bas… 2022-10-01  wk in…       1 US       quantile             0.4     145 #> # ℹ 128 more rows mod_out_dir_cloud <- hubData::s3_bucket(   \"hubverse/hubutils/testhubs/simple/model-output/\" ) mod_out_con_cloud <- hubData::connect_model_output(   mod_out_dir_cloud,   file_format = \"csv\" ) mod_out_con_cloud #>  #> ── <mod_out_connection/FileSystemDataset> ── #>  #> • file_format: \"csv(3/3)\" #> • checks: TRUE #> • file_system: \"S3FileSystem\" #> • model_output_dir: \"hubverse/hubutils/testhubs/simple/model-output/\" #>  #> ── Connection schema #> mod_out_connection with 3 csv files #> 8 columns #> origin_date: date32[day] #> target: string #> horizon: int64 #> location: string #> output_type: string #> output_type_id: double #> value: int64 #> model_id: string mod_out_dir_cloud <- hubData::s3_bucket(   \"hubverse/hubutils/testhubs/parquet/model-output/\" ) mod_out_con_cloud <- hubData::connect_model_output(   mod_out_dir_cloud,   file_format = \"parquet\",   skip_checks = TRUE ) mod_out_con_cloud #>  #> ── <mod_out_connection/FileSystemDataset> ── #>  #> • file_format: \"parquet(4/4)\" #> • checks: FALSE #> • file_system: \"S3FileSystem\" #> • model_output_dir: \"hubverse/hubutils/testhubs/parquet/model-output/\" #>  #> ── Connection schema #> mod_out_connection with 4 Parquet files #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> age_group: string #> model_id: string"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"providing-a-custom-schema","dir":"Articles","previous_headings":"Connecting to a model output directory","what":"Providing a custom schema","title":"Accessing Model Output Data","text":"connecting model output directly, can also specify schema override default arrow schema auto-detection. can help times resolve conflicts data types across different dataset files. Using schema can however also produce new errors can sometimes hard debug. example, defining schema field output_type cast int32 data type. column output_type actually contain character type data coerced integer, connecting model output directory produces arrow error. Beware arrow errors can somewhat misleading times get non-informative error, good place start check schema matches columns data can coerced data types specified schema.","code":"library(arrow) #>  #> Attaching package: 'arrow' #> The following object is masked from 'package:utils': #>  #>     timestamp  model_output_schema <- arrow::schema(   origin_date = date32(),   target = string(),   horizon = int32(),   location = string(),   output_type = string(),   output_type_id = string(),   value = int32(),   model_id = string() )  mod_out_con <- hubData::connect_model_output(   model_output_dir,   file_format = \"csv\",   schema = model_output_schema ) mod_out_con #>  #> ── <mod_out_connection/FileSystemDataset> ── #>  #> • file_format: \"csv(3/3)\" #> • checks: TRUE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #>  #> ── Connection schema #> mod_out_connection with 3 csv files #> 8 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: string #> value: int32 #> model_id: string model_output_schema <- arrow::schema(   origin_date = date32(),   target = string(),   horizon = int32(),   location = string(),   output_type = int32(),   output_type_id = string(),   value = int32(),   model_id = string() )  mod_out_con <- hubData::connect_model_output(   model_output_dir,   file_format = \"csv\",   schema = model_output_schema ) #> Error in `arrow::open_dataset()`: #> ! Invalid: No non-null segments were available for field 'model_id'; couldn't infer type"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_hub.html","id":"working-with-target-data","dir":"Articles","previous_headings":"","what":"Working with target data","title":"Accessing Model Output Data","text":"connect_hub() connect_model_output() focus accessing model predictions, modeling hubs also contain target data - “ground truth” observations models trying predict. hubData provides dedicated functions accessing target data: connect_target_timeseries() - Access historical time-series target observations connect_target_oracle_output() - Access target data formatted like model outputs (useful evaluation) functions work similarly connect_hub(), returning Arrow datasets can query dplyr verbs collect collect(). detailed examples best practices working target data, including join target data model outputs evaluation, see vignette(\"connect_target_data\").","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Accessing Target Data","text":"Target data represents “ground truth” observations models trying predict forecasting hubs. Understanding access work target data essential evaluating model performance, creating visualizations, conducting analyses. comprehensive overview target data matters context modeling hubs, please refer Hubverse target data guide. vignette focuses practical aspects accessing target data using hubData’s specialized functions: Time-series target data: Historical observations (stored target-data/time-series.csv, target-data/time-series.parquet, target-data/time-series/ directory) Oracle-output target data: Model-formatted target observations (stored target-data/oracle-output.csv, target-data/oracle-output.parquet, target-data/oracle-output/ directory)","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"target-data-structure","dir":"Articles","previous_headings":"","what":"Target data structure","title":"Accessing Target Data","text":"Modeling hubs store target data two complementary formats, serving distinct purposes forecasting workflow:","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"time-series-format","dir":"Articles","previous_headings":"Target data structure","what":"Time-series format","title":"Accessing Target Data","text":"Time-series target data represents observed counts rates native format. row constitutes observable unit - unique combination task ID variables defines single observation. format typically includes: date variable Location identifiers observation column containing measured value (e.g., case counts, hospitalization numbers) task ID variables example, task IDs location date, observable unit specific location-date combination (e.g., “US 2022-10-15”), observation column contains value measured unit. time-series format? format designed : Model fitting: Providing historical data parameter estimation model training Visualization: Supporting tools like hubVis dashboards display historical trends General analysis: Working target data natural, observational form","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"oracle-output-format","dir":"Articles","previous_headings":"Target data structure","what":"Oracle-output format","title":"Accessing Target Data","text":"Oracle-output target data generally derived time-series data formatted match structure model outputs (similar model_out_tbl objects). represents “predictions look like target values known ahead time.” Like time-series data, row represents observable unit defined task ID variables, data structured model output. format includes: output_type output_type_id: required hub collects pmf cdf output types. mean, median, quantile, sample output types, columns can omitted entirely. oracle_value: target observation value formatted “oracle” prediction Task ID variables (e.g., location, date) oracle-output format? format designed : Model evaluation: Enabling evaluation tools like hubEvals directly compare model predictions observed outcomes Visualization: Supporting plots display predictions alongside target data consistent format (e.g., plotting pmf predictions categorical target data) formatting target data model output probability mass observed outcome, oracle-output data allows evaluation tools treat identically model predictions.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"connection-approach","dir":"Articles","previous_headings":"","what":"Connection approach","title":"Accessing Target Data","text":"Accessing target data follows approach accessing model outputs (see vignette(\"connect_hub\") details): Connections established arrow package, opening datasets FileSystemDatasets connect_target_timeseries() connect_target_oracle_output() work data stored locally cloud (e.g., AWS S3 buckets) functions use hub configuration files determine appropriate schema target data connected, can use dplyr verbs filter query data collecting memory means workflows use model output data also apply target data.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"accessing-time-series-target-data","dir":"Articles","previous_headings":"","what":"Accessing time-series target data","title":"Accessing Target Data","text":"First, load required packages: Use connect_target_timeseries() open connection time-series target data: shows Arrow dataset structure, including schema (column names data types) information underlying file(s). connection lazy - data loaded memory explicitly collect . can query collect data connection using dplyr verbs:","code":"library(hubData) library(dplyr) #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union hub_path <- system.file(\"testhubs/v6/target_dir\", package = \"hubUtils\") ts_con <- connect_target_timeseries(hub_path) ts_con #> target_timeseries with 2 Parquet files #> 4 columns #> target_end_date: date32[day] #> target: string #> location: string #> observation: double # Collect all time-series data ts_con |>   collect() #> # A tibble: 66 × 4 #>    target_end_date target           location observation #>    <date>          <chr>            <chr>          <dbl> #>  1 2022-10-22      wk flu hosp rate 02             0.422 #>  2 2022-10-22      wk flu hosp rate 01             2.78  #>  3 2022-10-22      wk flu hosp rate US             0.716 #>  4 2022-10-29      wk flu hosp rate 02             1.97  #>  5 2022-10-29      wk flu hosp rate 01             5.17  #>  6 2022-10-29      wk flu hosp rate US             1.31  #>  7 2022-11-05      wk flu hosp rate 02             1.41  #>  8 2022-11-05      wk flu hosp rate 01             7.11  #>  9 2022-11-05      wk flu hosp rate US             1.98  #> 10 2022-11-12      wk flu hosp rate 02             2.81  #> # ℹ 56 more rows"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"filtering-time-series-data","dir":"Articles","previous_headings":"Accessing time-series target data","what":"Filtering time-series data","title":"Accessing Target Data","text":"can filter collecting work subsets data:","code":"# Filter by location ts_con |>   filter(location == \"US\") |>   collect() #> # A tibble: 22 × 4 #>    target_end_date target          location observation #>    <date>          <chr>           <chr>          <dbl> #>  1 2022-10-22      wk inc flu hosp US              2380 #>  2 2022-10-29      wk inc flu hosp US              4353 #>  3 2022-11-05      wk inc flu hosp US              6571 #>  4 2022-11-12      wk inc flu hosp US              8848 #>  5 2022-11-19      wk inc flu hosp US             11427 #>  6 2022-11-26      wk inc flu hosp US             19846 #>  7 2022-12-03      wk inc flu hosp US             26333 #>  8 2022-12-10      wk inc flu hosp US             23851 #>  9 2022-12-17      wk inc flu hosp US             21435 #> 10 2022-12-24      wk inc flu hosp US             19286 #> # ℹ 12 more rows  # Filter by date range ts_con |>   filter(target_end_date >= \"2022-10-01\") |>   collect() #> # A tibble: 66 × 4 #>    target_end_date target           location observation #>    <date>          <chr>            <chr>          <dbl> #>  1 2022-10-22      wk flu hosp rate 02             0.422 #>  2 2022-10-22      wk flu hosp rate 01             2.78  #>  3 2022-10-22      wk flu hosp rate US             0.716 #>  4 2022-10-29      wk flu hosp rate 02             1.97  #>  5 2022-10-29      wk flu hosp rate 01             5.17  #>  6 2022-10-29      wk flu hosp rate US             1.31  #>  7 2022-11-05      wk flu hosp rate 02             1.41  #>  8 2022-11-05      wk flu hosp rate 01             7.11  #>  9 2022-11-05      wk flu hosp rate US             1.98  #> 10 2022-11-12      wk flu hosp rate 02             2.81  #> # ℹ 56 more rows  # Combine multiple filters ts_con |>   filter(     location %in% c(\"US\", \"01\"),     target_end_date >= \"2022-10-01\"   ) |>   collect() #> # A tibble: 44 × 4 #>    target_end_date target           location observation #>    <date>          <chr>            <chr>          <dbl> #>  1 2022-10-22      wk flu hosp rate 01             2.78  #>  2 2022-10-22      wk flu hosp rate US             0.716 #>  3 2022-10-29      wk flu hosp rate 01             5.17  #>  4 2022-10-29      wk flu hosp rate US             1.31  #>  5 2022-11-05      wk flu hosp rate 01             7.11  #>  6 2022-11-05      wk flu hosp rate US             1.98  #>  7 2022-11-12      wk flu hosp rate 01             5.98  #>  8 2022-11-12      wk flu hosp rate US             2.66  #>  9 2022-11-19      wk flu hosp rate 01             4.46  #> 10 2022-11-19      wk flu hosp rate US             3.44  #> # ℹ 34 more rows"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"accessing-oracle-output-target-data","dir":"Articles","previous_headings":"","what":"Accessing oracle-output target data","title":"Accessing Target Data","text":"Use connect_target_oracle_output() open connection oracle-output target data: Like time-series connection, displays Arrow dataset structure schema showing columns formatted like model outputs. Notice presence columns like output_type, output_type_id, oracle_value. can query collect data connection:","code":"oo_con <- connect_target_oracle_output(hub_path) oo_con #> target_oracle_output with 5 Parquet files #> 6 columns #> target_end_date: date32[day] #> target: string #> location: string #> output_type: string #> output_type_id: string #> oracle_value: double # Collect all oracle-output data oo_con |>   collect() #> # A tibble: 627 × 6 #>    target_end_date target       location output_type output_type_id oracle_value #>    <date>          <chr>        <chr>    <chr>       <chr>                 <dbl> #>  1 2022-10-22      wk flu hosp… US       cdf         1                         1 #>  2 2022-10-22      wk flu hosp… US       cdf         2                         1 #>  3 2022-10-22      wk flu hosp… US       cdf         3                         1 #>  4 2022-10-22      wk flu hosp… US       cdf         4                         1 #>  5 2022-10-22      wk flu hosp… US       cdf         5                         1 #>  6 2022-10-22      wk flu hosp… US       cdf         6                         1 #>  7 2022-10-22      wk flu hosp… US       cdf         7                         1 #>  8 2022-10-22      wk flu hosp… US       cdf         8                         1 #>  9 2022-10-22      wk flu hosp… US       cdf         9                         1 #> 10 2022-10-22      wk flu hosp… US       cdf         10                        1 #> # ℹ 617 more rows"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"filtering-oracle-output-data","dir":"Articles","previous_headings":"Accessing oracle-output target data","what":"Filtering oracle-output data","title":"Accessing Target Data","text":"Oracle-output data structure model output, making easy filter output type: Notice output_type_id NA quantile outputs. oracle value represents observed outcome probability mass concentrated single value - oracle_value applies quantile levels. pmf cdf output types, output_type_id specify category threshold.","code":"# Get quantile forecasts only oo_con |>   filter(output_type == \"quantile\") |>   collect() #> # A tibble: 33 × 6 #>    target_end_date target       location output_type output_type_id oracle_value #>    <date>          <chr>        <chr>    <chr>       <chr>                 <dbl> #>  1 2022-10-22      wk inc flu … US       quantile    NA                     2380 #>  2 2022-10-22      wk inc flu … 01       quantile    NA                      141 #>  3 2022-10-22      wk inc flu … 02       quantile    NA                        3 #>  4 2022-10-29      wk inc flu … US       quantile    NA                     4353 #>  5 2022-10-29      wk inc flu … 01       quantile    NA                      262 #>  6 2022-10-29      wk inc flu … 02       quantile    NA                       14 #>  7 2022-11-05      wk inc flu … US       quantile    NA                     6571 #>  8 2022-11-05      wk inc flu … 01       quantile    NA                      360 #>  9 2022-11-05      wk inc flu … 02       quantile    NA                       10 #> 10 2022-11-12      wk inc flu … US       quantile    NA                     8848 #> # ℹ 23 more rows # Get specific pmf category oo_con |>   filter(     output_type == \"pmf\",     output_type_id == \"large\"   ) |>   collect() #> # A tibble: 0 × 6 #> # ℹ 6 variables: target_end_date <date>, target <chr>, location <chr>, #> #   output_type <chr>, output_type_id <chr>, oracle_value <dbl>  # Filter by location and date oo_con |>   filter(     location == \"US\",     target_end_date == \"2022-12-31\"   ) |>   collect() #> # A tibble: 19 × 6 #>    target_end_date target       location output_type output_type_id oracle_value #>    <date>          <chr>        <chr>    <chr>       <chr>                 <dbl> #>  1 2022-12-31      wk flu hosp… US       cdf         1                         0 #>  2 2022-12-31      wk flu hosp… US       cdf         2                         0 #>  3 2022-12-31      wk flu hosp… US       cdf         3                         0 #>  4 2022-12-31      wk flu hosp… US       cdf         4                         0 #>  5 2022-12-31      wk flu hosp… US       cdf         5                         0 #>  6 2022-12-31      wk flu hosp… US       cdf         6                         1 #>  7 2022-12-31      wk flu hosp… US       cdf         7                         1 #>  8 2022-12-31      wk flu hosp… US       cdf         8                         1 #>  9 2022-12-31      wk flu hosp… US       cdf         9                         1 #> 10 2022-12-31      wk flu hosp… US       cdf         10                        1 #> 11 2022-12-31      wk flu hosp… US       cdf         11                        1 #> 12 2022-12-31      wk flu hosp… US       cdf         12                        1 #> 13 2022-12-31      wk inc flu … US       mean        NA                    19369 #> 14 2022-12-31      wk inc flu … US       quantile    NA                    19369 #> 15 2022-12-31      wk flu hosp… US       pmf         low                       0 #> 16 2022-12-31      wk flu hosp… US       pmf         moderate                  0 #> 17 2022-12-31      wk flu hosp… US       pmf         high                      1 #> 18 2022-12-31      wk flu hosp… US       pmf         very high                 0 #> 19 2022-12-31      wk inc flu … US       sample      NA                    19369"},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"joining-target-data-with-model-outputs","dir":"Articles","previous_headings":"Working with target data","what":"Joining target data with model outputs","title":"Accessing Target Data","text":"common workflow join oracle-output target data model predictions evaluation. Let’s start connecting model outputs collecting predictions: Next, collect corresponding oracle-output target data: joining, need remove output_type output_type_id columns oracle-output data. quantile (mean, median, sample) outputs, columns don’t provide useful information since oracle value applies across quantile levels. Keeping cause merge conflicts. Note: pmf cdf output types, need keep columns specify category threshold predicted. Now successfully aligned predicted values (value) target observations (oracle_value) combination task IDs.","code":"# Connect to model outputs hub_con <- connect_hub(hub_path)  # Collect model outputs for a specific location and output type model_data <- hub_con |>   filter(     output_type == \"quantile\",     location == \"US\"   ) |>   collect_hub()  model_data #> # A tibble: 132 × 9 #>    model_id   location reference_date horizon target_end_date target output_type #>  * <chr>      <chr>    <date>           <int> <date>          <chr>  <chr>       #>  1 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  2 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  3 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  4 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  5 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  6 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  7 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  8 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  9 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #> 10 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #> # ℹ 122 more rows #> # ℹ 2 more variables: output_type_id <chr>, value <dbl> # Collect corresponding oracle-output target data target_data <- oo_con |>   filter(     output_type == \"quantile\",     location == \"US\"   ) |>   collect()  target_data #> # A tibble: 11 × 6 #>    target_end_date target       location output_type output_type_id oracle_value #>    <date>          <chr>        <chr>    <chr>       <chr>                 <dbl> #>  1 2022-10-22      wk inc flu … US       quantile    NA                     2380 #>  2 2022-10-29      wk inc flu … US       quantile    NA                     4353 #>  3 2022-11-05      wk inc flu … US       quantile    NA                     6571 #>  4 2022-11-12      wk inc flu … US       quantile    NA                     8848 #>  5 2022-11-19      wk inc flu … US       quantile    NA                    11427 #>  6 2022-11-26      wk inc flu … US       quantile    NA                    19846 #>  7 2022-12-03      wk inc flu … US       quantile    NA                    26333 #>  8 2022-12-10      wk inc flu … US       quantile    NA                    23851 #>  9 2022-12-17      wk inc flu … US       quantile    NA                    21435 #> 10 2022-12-24      wk inc flu … US       quantile    NA                    19286 #> 11 2022-12-31      wk inc flu … US       quantile    NA                    19369 # Remove unnecessary columns that would cause merge conflicts target_data <- target_data |>   select(-c(output_type, output_type_id))  # Join on common task ID columns join_cols <- c(\"location\", \"target_end_date\", \"target\")  comparison <- model_data |>   inner_join(     target_data,     by = join_cols   )  comparison #> # A tibble: 132 × 10 #>    model_id   location reference_date horizon target_end_date target output_type #>    <chr>      <chr>    <date>           <int> <date>          <chr>  <chr>       #>  1 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  2 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  3 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  4 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  5 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  6 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  7 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  8 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #>  9 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #> 10 Flusight-… US       2022-10-22           1 2022-10-29      wk in… quantile    #> # ℹ 122 more rows #> # ℹ 3 more variables: output_type_id <chr>, value <dbl>, oracle_value <dbl>"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"special-case-hubs-with-horizon-based-forecasts","dir":"Articles","previous_headings":"Working with target data > Joining target data with model outputs","what":"Special case: Hubs with horizon-based forecasts","title":"Accessing Target Data","text":"hubs collect forecasts using reference date (origin date) horizon column, rather explicitly storing target end date. cases, target end date often calculated origin_date + (horizon * 7L) (assuming weekly forecasts). working hubs, ’ll need calculate target_end_date model output data joining target data: Now can join target data : Note: multiplier (7L) assumes weekly horizons. Adjust based hub’s configuration (e.g., 1L daily horizons).","code":"# Model data without target_end_date model_data_horizon #> # A tibble: 132 × 8 #>    model_id    location reference_date horizon target output_type output_type_id #>    <chr>       <chr>    <date>           <int> <chr>  <chr>       <chr>          #>  1 Flusight-b… US       2022-10-22           1 wk in… quantile    0.025          #>  2 Flusight-b… US       2022-10-22           1 wk in… quantile    0.1            #>  3 Flusight-b… US       2022-10-22           1 wk in… quantile    0.2            #>  4 Flusight-b… US       2022-10-22           1 wk in… quantile    0.3            #>  5 Flusight-b… US       2022-10-22           1 wk in… quantile    0.4            #>  6 Flusight-b… US       2022-10-22           1 wk in… quantile    0.5            #>  7 Flusight-b… US       2022-10-22           1 wk in… quantile    0.6            #>  8 Flusight-b… US       2022-10-22           1 wk in… quantile    0.7            #>  9 Flusight-b… US       2022-10-22           1 wk in… quantile    0.8            #> 10 Flusight-b… US       2022-10-22           1 wk in… quantile    0.9            #> # ℹ 122 more rows #> # ℹ 1 more variable: value <dbl> # Calculate target_end_date from origin_date and horizon model_data_horizon <- model_data_horizon |>   mutate(     target_end_date = reference_date + (horizon * 7L)   )  model_data_horizon #> # A tibble: 132 × 9 #>    model_id    location reference_date horizon target output_type output_type_id #>    <chr>       <chr>    <date>           <int> <chr>  <chr>       <chr>          #>  1 Flusight-b… US       2022-10-22           1 wk in… quantile    0.025          #>  2 Flusight-b… US       2022-10-22           1 wk in… quantile    0.1            #>  3 Flusight-b… US       2022-10-22           1 wk in… quantile    0.2            #>  4 Flusight-b… US       2022-10-22           1 wk in… quantile    0.3            #>  5 Flusight-b… US       2022-10-22           1 wk in… quantile    0.4            #>  6 Flusight-b… US       2022-10-22           1 wk in… quantile    0.5            #>  7 Flusight-b… US       2022-10-22           1 wk in… quantile    0.6            #>  8 Flusight-b… US       2022-10-22           1 wk in… quantile    0.7            #>  9 Flusight-b… US       2022-10-22           1 wk in… quantile    0.8            #> 10 Flusight-b… US       2022-10-22           1 wk in… quantile    0.9            #> # ℹ 122 more rows #> # ℹ 2 more variables: value <dbl>, target_end_date <date> join_cols <- c(\"location\", \"target_end_date\", \"target\")  comparison_horizon <- model_data_horizon |>   inner_join(     target_data,     by = join_cols   )  comparison_horizon #> # A tibble: 132 × 10 #>    model_id    location reference_date horizon target output_type output_type_id #>    <chr>       <chr>    <date>           <int> <chr>  <chr>       <chr>          #>  1 Flusight-b… US       2022-10-22           1 wk in… quantile    0.025          #>  2 Flusight-b… US       2022-10-22           1 wk in… quantile    0.1            #>  3 Flusight-b… US       2022-10-22           1 wk in… quantile    0.2            #>  4 Flusight-b… US       2022-10-22           1 wk in… quantile    0.3            #>  5 Flusight-b… US       2022-10-22           1 wk in… quantile    0.4            #>  6 Flusight-b… US       2022-10-22           1 wk in… quantile    0.5            #>  7 Flusight-b… US       2022-10-22           1 wk in… quantile    0.6            #>  8 Flusight-b… US       2022-10-22           1 wk in… quantile    0.7            #>  9 Flusight-b… US       2022-10-22           1 wk in… quantile    0.8            #> 10 Flusight-b… US       2022-10-22           1 wk in… quantile    0.9            #> # ℹ 122 more rows #> # ℹ 3 more variables: value <dbl>, target_end_date <date>, oracle_value <dbl>"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"computing-simple-metrics","dir":"Articles","previous_headings":"Working with target data","what":"Computing simple metrics","title":"Accessing Target Data","text":"model predictions target data, can compute evaluation metrics: workflow aligning predictions target observations computing metrics exactly evaluation tools like hubEvals automate part comprehensive model evaluation pipelines.","code":"# Calculate absolute error for median forecasts comparison |>   filter(output_type_id == \"0.5\") |>   mutate(     abs_error = abs(value - oracle_value)   ) |>   group_by(model_id) |>   summarise(     mean_abs_error = mean(abs_error, na.rm = TRUE),     .groups = \"drop\"   ) #> # A tibble: 3 × 2 #>   model_id          mean_abs_error #>   <chr>                      <dbl> #> 1 Flusight-baseline          9050. #> 2 MOBS-GLEAM_FLUH            5475. #> 3 PSI-DICE                   6526."},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"accessing-target-data-from-cloud-hubs","dir":"Articles","previous_headings":"","what":"Accessing target data from cloud hubs","title":"Accessing Target Data","text":"connect_target_timeseries() connect_target_oracle_output() work seamlessly cloud-based hubs. Use cloud connection approach connect_hub():","code":"# Connect to a cloud hub s3_hub_path <- s3_bucket(\"example-complex-forecast-hub\")  # Access time-series target data from cloud ts_cloud <- connect_target_timeseries(s3_hub_path) ts_cloud #> target_timeseries with 1 csv file #> 4 columns #> target_end_date: date32[day] #> target: string #> location: string #> observation: double # Collect a sample of the data ts_cloud |>   filter(location == \"US\") |>   collect() #> # A tibble: 402 × 4 #>    target_end_date target          location observation #>    <date>          <chr>           <chr>          <dbl> #>  1 2020-01-11      wk inc flu hosp US                 1 #>  2 2020-01-18      wk inc flu hosp US                 0 #>  3 2020-01-25      wk inc flu hosp US                 0 #>  4 2020-02-01      wk inc flu hosp US                 0 #>  5 2020-02-08      wk inc flu hosp US                 0 #>  6 2020-02-15      wk inc flu hosp US                 0 #>  7 2020-02-22      wk inc flu hosp US                 0 #>  8 2020-02-29      wk inc flu hosp US                 0 #>  9 2020-03-07      wk inc flu hosp US                 0 #> 10 2020-03-14      wk inc flu hosp US                 0 #> # ℹ 392 more rows # Access oracle-output target data from cloud oo_cloud <- connect_target_oracle_output(s3_hub_path) oo_cloud #> target_oracle_output with 1 csv file #> 6 columns #> location: string #> target_end_date: date32[day] #> target: string #> output_type: string #> output_type_id: string #> oracle_value: double # Collect a sample of oracle-output data oo_cloud |>   filter(location == \"US\") |>   collect() #> # A tibble: 3,780 × 6 #>    location target_end_date target       output_type output_type_id oracle_value #>    <chr>    <date>          <chr>        <chr>       <chr>                 <dbl> #>  1 US       2022-10-22      wk inc flu … quantile    NA                     2380 #>  2 US       2022-10-29      wk inc flu … quantile    NA                     4353 #>  3 US       2022-11-05      wk inc flu … quantile    NA                     6571 #>  4 US       2022-11-12      wk inc flu … quantile    NA                     8848 #>  5 US       2022-11-19      wk inc flu … quantile    NA                    11427 #>  6 US       2022-11-26      wk inc flu … quantile    NA                    19846 #>  7 US       2022-12-03      wk inc flu … quantile    NA                    26333 #>  8 US       2022-12-10      wk inc flu … quantile    NA                    23851 #>  9 US       2022-12-17      wk inc flu … quantile    NA                    21435 #> 10 US       2022-12-24      wk inc flu … quantile    NA                    19286 #> # ℹ 3,770 more rows"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"performance-tips-for-cloud-hubs","dir":"Articles","previous_headings":"Accessing target data from cloud hubs","what":"Performance tips for cloud hubs","title":"Accessing Target Data","text":"working cloud-based target data, consider performance tips: Filter collecting: Always apply filters Arrow dataset calling collect() minimize data transfer: Select specific columns: need certain columns, use select() collecting:","code":"# Good: filter first, then collect ts_cloud |>   filter(location == \"US\") |>   collect()  # Less efficient: collect everything, then filter ts_cloud |>   collect() |>   filter(location == \"US\") ts_cloud |>   select(location, target_end_date, observation) |>   filter(location == \"US\") |>   collect() #> # A tibble: 402 × 3 #>    location target_end_date observation #>    <chr>    <date>                <dbl> #>  1 US       2020-01-11                1 #>  2 US       2020-01-18                0 #>  3 US       2020-01-25                0 #>  4 US       2020-02-01                0 #>  5 US       2020-02-08                0 #>  6 US       2020-02-15                0 #>  7 US       2020-02-22                0 #>  8 US       2020-02-29                0 #>  9 US       2020-03-07                0 #> 10 US       2020-03-14                0 #> # ℹ 392 more rows"},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"hub-version-compatibility","dir":"Articles","previous_headings":"","what":"Hub version compatibility","title":"Accessing Target Data","text":"target data functions work transparently across different hub versions: Newer hubs (v6+) target-data.json configuration benefit optimized schema creation potentially better performance Older hubs without target-data.json schemas inferred automatically data files user, don’t need worry implementation details - API works hubs, functions automatically detect use appropriate method.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/articles/connect_target_data.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Accessing Target Data","text":"Use connect_target_timeseries() historical observational data Use connect_target_oracle_output() model-formatted target data functions return Arrow datasets work dplyr verbs Filter collecting improve performance, especially cloud hubs Oracle-output format makes easy join model predictions evaluation API works across hub versions information : - Model output data, see vignette(\"connect_hub\") - Target data concepts, see Hubverse target data guide","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Anna Krystalli. Author, maintainer. Li Shandross. Contributor. Nicholas G. Reich. Contributor. Evan L. Ray. Contributor. Becky Sweger. Contributor. Consortium Infectious Disease Modeling Hubs. Copyright holder.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Krystalli (2025). hubData: Tools accessing working hubverse data. R package version 1.5.0.9000, https://hubverse-org.github.io/hubData.","code":"@Manual{,   title = {hubData: Tools for accessing and working with hubverse data},   author = {Anna Krystalli},   year = {2025},   note = {R package version 1.5.0.9000},   url = {https://hubverse-org.github.io/hubData}, }"},{"path":"https://hubverse-org.github.io/hubData/dev/index.html","id":"hubdata-","dir":"","previous_headings":"","what":"Tools for accessing and working with hubverse data","title":"Tools for accessing and working with hubverse data","text":"goal hubData provide tools accessing working hubverse Hub data. package part hubverse ecosystem, aims provide set tools infectious disease modeling hubs share collaborate work.","code":""},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/index.html","id":"latest","dir":"","previous_headings":"Installation","what":"Latest","title":"Tools for accessing and working with hubverse data","text":"can install latest version hubData R-universe:","code":"install.packages(\"hubData\", repos = c(\"https://hubverse-org.r-universe.dev\", \"https://cloud.r-project.org\"))"},{"path":"https://hubverse-org.github.io/hubData/dev/index.html","id":"development","dir":"","previous_headings":"Installation","what":"Development","title":"Tools for accessing and working with hubverse data","text":"want test new features yet released, can install development version hubData GitHub : [!NOTE] hubData dependency arrow package. troubleshooting arrow installation problems, please consult arrow package documentation. also try installing package Apache R Universe repository :","code":"# install.packages(\"remotes\") remotes::install_github(\"hubverse-org/hubData\") install.packages(\"arrow\", repos = c(\"https://apache.r-universe.dev\", \"https://cran.r-project.org\"))"},{"path":"https://hubverse-org.github.io/hubData/dev/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Tools for accessing and working with hubverse data","text":"Please note hubData package released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tools for accessing and working with hubverse data","text":"Interested contributing back open-source Hubverse project? Learn get involved Hubverse Community contribute hubData package.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/arrow_to_r_datatypes.html","id":null,"dir":"Reference","previous_headings":"","what":"Mapping of Arrow types to base R types — arrow_to_r_datatypes","title":"Mapping of Arrow types to base R types — arrow_to_r_datatypes","text":"named character vector mapping common arrow::Schema field types (strings) corresponding base R types. mapping used translate validate column types working Parquet files Arrow datasets, especially schema inference compatibility checks.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/arrow_to_r_datatypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mapping of Arrow types to base R types — arrow_to_r_datatypes","text":"","code":"arrow_to_r_datatypes"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/arrow_to_r_datatypes.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Mapping of Arrow types to base R types — arrow_to_r_datatypes","text":"named character vector 8 entries.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/arrow_to_r_datatypes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mapping of Arrow types to base R types — arrow_to_r_datatypes","text":"safest portable Arrow types supported hubverse. Types present mapping treated unsupported.","code":""},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/reference/as_r_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","title":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","text":"functions help convert validate arrow::Schema object (typically Parquet file Arrow dataset) translating Arrow types R equivalents, extracting type strings, checking compatibility.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/as_r_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","text":"","code":"as_r_schema(arrow_schema, call = rlang::caller_env())  arrow_schema_to_string(arrow_schema)  is_supported_arrow_type(arrow_schema)  validate_arrow_schema(arrow_schema, call = rlang::caller_env())"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/as_r_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","text":"arrow_schema arrow::Schema object, one returned arrow::read_parquet(..., as_data_frame = FALSE)$schema arrow::open_dataset(...)$schema. call calling environment, used error reporting validate_arrow_schema() as_r_schema() (default: caller's environment).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/as_r_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","text":"as_r_schema(): named character vector mapping column names base R type strings (e.g., \"integer\", \"double\", \"logical\"). arrow_schema_to_string(): named character vector mapping column names Arrow type strings. is_supported_arrow_type(): named logical vector indicating whether column supported. validate_arrow_schema(): Returns original schema (invisibly) column types supported; otherwise throws error.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/as_r_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","text":"as_r_schema() maps Arrow types base R types (e.g., \"int32\" → \"integer\"). throws error unsupported column types present. arrow_schema_to_string() returns named character vector raw Arrow type strings (e.g., \"int64\", \"date32[day]\") schema field. is_supported_arrow_type() returns named logical vector indicating whether schema field type supported. validate_arrow_schema() throws error fields unsupported Arrow type. full list supported types R mappings, see arrow_to_r_datatypes().","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/as_r_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert or validate an Arrow schema for compatibility with base R column types — as_r_schema","text":"","code":"# Path to a single Parquet file file_path <- system.file(   \"testhubs/parquet/model-output/hub-baseline/2022-10-01-hub-baseline.parquet\",   package = \"hubUtils\" )  # Get schema from the file file_schema <- arrow::read_parquet(file_path, as_data_frame = FALSE)$schema  # Convert to R types as_r_schema(file_schema) #>    origin_date         target        horizon       location    output_type  #>         \"Date\"    \"character\"      \"integer\"    \"character\"    \"character\"  #> output_type_id          value  #>       \"double\"      \"integer\"   # Get raw Arrow type strings arrow_schema_to_string(file_schema) #>    origin_date         target        horizon       location    output_type  #>  \"date32[day]\"       \"string\"        \"int32\"       \"string\"       \"string\"  #> output_type_id          value  #>       \"double\"        \"int32\"   # Check which columns are supported is_supported_arrow_type(file_schema) #>    origin_date         target        horizon       location    output_type  #>           TRUE           TRUE           TRUE           TRUE           TRUE  #> output_type_id          value  #>           TRUE           TRUE   # Validate schema (throws error if any unsupported types are present) validate_arrow_schema(file_schema)  # From a multi-file dataset dataset_path <- system.file(   \"testhubs/parquet/model-output/hub-baseline\",   package = \"hubUtils\" ) ds <- arrow::open_dataset(dataset_path) as_r_schema(ds$schema) #>    origin_date         target        horizon       location    output_type  #>         \"Date\"    \"character\"      \"integer\"    \"character\"    \"character\"  #> output_type_id          value  #>       \"double\"      \"integer\"  arrow_schema_to_string(ds$schema) #>    origin_date         target        horizon       location    output_type  #>  \"date32[day]\"       \"string\"        \"int32\"       \"string\"       \"string\"  #> output_type_id          value  #>       \"double\"        \"int32\"  is_supported_arrow_type(ds$schema) #>    origin_date         target        horizon       location    output_type  #>           TRUE           TRUE           TRUE           TRUE           TRUE  #> output_type_id          value  #>           TRUE           TRUE  validate_arrow_schema(ds$schema)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/coerce_to_hub_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce data.frame/tibble column data types to hub schema data types or character. — coerce_to_hub_schema","title":"Coerce data.frame/tibble column data types to hub schema data types or character. — coerce_to_hub_schema","text":"Coerce data.frame/tibble column data types hub schema data types character.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/coerce_to_hub_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce data.frame/tibble column data types to hub schema data types or character. — coerce_to_hub_schema","text":"","code":"coerce_to_hub_schema(   tbl,   config_tasks,   skip_date_coercion = FALSE,   as_arrow_table = FALSE,   output_type_id_datatype = c(\"from_config\", \"auto\", \"character\", \"double\", \"integer\",     \"logical\", \"Date\") )  coerce_to_character(tbl, as_arrow_table = FALSE)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/coerce_to_hub_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce data.frame/tibble column data types to hub schema data types or character. — coerce_to_hub_schema","text":"tbl model output data.frame/tibble config_tasks list version content's hub's tasks.json config file created using function hubUtils::read_config(). skip_date_coercion Logical. Whether skip coercing dates. can faster, especially larger tbls. as_arrow_table Logical. Whether return arrow table. Defaults FALSE. output_type_id_datatype character string. One \"from_config\", \"auto\", \"character\", \"double\", \"integer\", \"logical\", \"Date\". Defaults \"from_config\" uses setting output_type_id_datatype property tasks.json config file available. property set config, argument falls back \"auto\" determines  output_type_id data type automatically tasks.json config file simplest data type required represent output type ID values across output types hub. point estimate output types (output_type_ids NA,) collected hub, output_type_id column assigned character data type auto-determined. data type values can used override automatic determination. Note attempting coerce output_type_id data type valid data (e.g. trying coerce\"character\" values \"double\") likely result error potentially unexpected behaviour use care.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/coerce_to_hub_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce data.frame/tibble column data types to hub schema data types or character. — coerce_to_hub_schema","text":"tbl column data types coerced hub schema data types character. as_arrow_table = TRUE, output also converted arrow table.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/coerce_to_hub_schema.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Coerce data.frame/tibble column data types to hub schema data types or character. — coerce_to_hub_schema","text":"coerce_to_hub_schema(): coerce columns hub schema data types. coerce_to_character(): coerce columns character","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_hub.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect Hub model output data — collect_hub","title":"Collect Hub model output data — collect_hub","text":"collect_hub retrieves data <hub_connection>/<mod_out_connection> executing <arrow_dplyr_query> local tibble. function also attempts convert output model_out_tbl class object returning.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_hub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect Hub model output data — collect_hub","text":"","code":"collect_hub(x, silent = FALSE, ...)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_hub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect Hub model output data — collect_hub","text":"x <hub_connection>/<mod_out_connection> <arrow_dplyr_query> object. silent Logical. Whether suppress message generated conversion model_out_tbl fails. ... argument passed as_model_out_tbl().","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_hub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect Hub model output data — collect_hub","text":"model_out_tbl, unless conversion model_out_tbl fails case tibble returned.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_hub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect Hub model output data — collect_hub","text":"","code":"hub_path <- system.file(\"testhubs/simple\", package = \"hubUtils\") hub_con <- connect_hub(hub_path) # Collect all data in a hub hub_con %>% collect_hub() #> # A tibble: 599 × 9 #>    model_id     origin_date target        horizon location age_group output_type #>  * <chr>        <date>      <chr>           <int> <chr>    <chr>     <chr>       #>  1 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        mean        #>  2 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  3 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  4 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  5 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  6 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  7 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  8 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #>  9 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #> 10 hub-baseline 2022-10-01  wk inc flu h…       1 US       NA        quantile    #> # ℹ 589 more rows #> # ℹ 2 more variables: output_type_id <dbl>, value <int> # Filter data before collecting hub_con %>%   dplyr::filter(is.na(output_type_id)) %>%   collect_hub() #> # A tibble: 1 × 9 #>   model_id     origin_date target         horizon location age_group output_type #> * <chr>        <date>      <chr>            <int> <chr>    <chr>     <chr>       #> 1 hub-baseline 2022-10-01  wk inc flu ho…       1 US       NA        mean        #> # ℹ 2 more variables: output_type_id <dbl>, value <int> # Pass arguments to as_model_out_tbl() dplyr::filter(hub_con, is.na(output_type_id)) %>%   collect_hub(remove_empty = TRUE) #> # A tibble: 1 × 8 #>   model_id  origin_date target horizon location output_type output_type_id value #> * <chr>     <date>      <chr>    <int> <chr>    <chr>                <dbl> <int> #> 1 hub-base… 2022-10-01  wk in…       1 US       mean                    NA   150"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_zoltar.html","id":null,"dir":"Reference","previous_headings":"","what":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","title":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","text":"collect_zoltar retrieves data zoltardata.com project transforms Zoltar's native download format hubverse one. Zoltar (documentation ) pre-hubverse research project implements repository model forecast results, including tools administer, query, visualize uploaded data, along R Python APIs access data programmatically (zoltr zoltpy, respectively.) (hubData function implemented using zoltr package.)","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_zoltar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","text":"","code":"collect_zoltar(   project_name,   models = NULL,   timezeros = NULL,   units = NULL,   targets = NULL,   types = NULL,   as_of = NULL,   point_output_type = \"median\" )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_zoltar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","text":"project_name string naming Zoltar project load forecasts . Assumes host zoltardata.com . models character vector specifies models query. Must model abbreviations. Defaults NULL, queries models project. timezeros character vector specifies timezeros query. Must yyyy-mm-dd format. Defaults NULL, queries timezeros project. units character vector specifies units query. Must unit abbreviations. Defaults NULL, queries units project. targets character vector specifies targets query. Must target names. Defaults NULL, queries targets project. types character vector specifies forecast types query. Choices \"bin\", \"point\", \"sample\", \"quantile\", \"mean\", \"median\". Defaults NULL, queries types project. Note: Zoltar supports \"named\" \"mode\" forecasts, function ignores . as_of datetime string specifies forecast version. datetime must include timezone information disambiguation, without query fail. datatime parsing function used (base::strftime) extremely lenient comes formatting, please exercise caution. Defaults NULL load latest version. point_output_type string specifies convert zoltar point forecast data hubverse output type. Must either \"median\" \"mean\". Defaults \"median\".","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_zoltar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","text":"hubverse model_out_tbl containing following columns: \"model_id\", \"timezero\", \"season\", \"unit\", \"horizon\", \"target\", \"output_type\", \"output_type_id\", \"value\".","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_zoltar.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","text":"Zoltar's data model differs hubverse important ways. Zoltar's model concepts unit, target, timezero, hubverse projects hub-configurable columns, makes mapping former latter imperfect. particular, Zoltar units translate roughly hubverse task IDs, Zoltar targets include target outcome numeric horizon target name, Zoltar timezeros map round ids. Finally, Zoltar's forecast types differ hubverse. Whereas Zoltar seven types (bin, named, point, sample, quantile, mean, median, mode), hubverse six (cdf, mean, median, pmf, quantile, sample), overlap. Additional notes: Requires user Zoltar account (use Zoltar contact page request one). Requires Z_USERNAME Z_PASSWORD environment vars set user's Zoltar account. Zoltar supports \"named\" \"mode\" forecasts, function ignores . Rows non-numeric values ignored. function removes numeric_horizon mentions zoltar target names. Target names can contain maximum one numeric_horizon. Example: \"1 wk ahead inc case\" -> \"wk ahead inc case\". Querying large number rows may cause errors, recommend providing one filtering arguments (e.g., models, timezeros, etc.) limit result.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/collect_zoltar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load forecasts from zoltardata.com in hubverse format — collect_zoltar","text":"","code":"if (FALSE) { # \\dontrun{ df <- collect_zoltar(\"Docs Example Project\") df <-   collect_zoltar(\"Docs Example Project\", models = c(\"docs_mod\"),                         timezeros = c(\"2011-10-16\"), units = c(\"loc1\", \"loc3\"),                         targets = c(\"pct next week\", \"cases next week\"), types = c(\"point\"),                         as_of = NULL, point_output_type = \"mean\") } # }"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to model output data. — connect_hub","title":"Connect to model output data. — connect_hub","text":"Connect data model output directory Modeling Hub directly. Data can stored local directory cloud AWS GCS.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to model output data. — connect_hub","text":"","code":"connect_hub(   hub_path,   file_format = c(\"csv\", \"parquet\", \"arrow\"),   output_type_id_datatype = c(\"from_config\", \"auto\", \"character\", \"double\", \"integer\",     \"logical\", \"Date\"),   partitions = list(model_id = arrow::utf8()),   skip_checks = TRUE,   na = c(\"NA\", \"\"),   ignore_files = NULL )  connect_model_output(   model_output_dir,   file_format = c(\"csv\", \"parquet\", \"arrow\"),   partition_names = \"model_id\",   schema = NULL,   skip_checks = FALSE,   na = c(\"NA\", \"\"),   ignore_files = NULL )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to model output data. — connect_hub","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. hub must fully configured valid admin.json tasks.json files within hub-config directory. file_format file format model output files stored . connection fully configured hub, accessed hub_path, file_format inferred hub's file_format configuration admin.json ignored default. supplied, override hub configuration setting. Multiple formats can supplied connect_hub single file format can supplied connect_model_output. output_type_id_datatype character string. One \"from_config\", \"auto\", \"character\", \"double\", \"integer\", \"logical\", \"Date\". Defaults \"from_config\" uses setting output_type_id_datatype property tasks.json config file available. property set config, argument falls back \"auto\" determines  output_type_id data type automatically tasks.json config file simplest data type required represent output type ID values across output types hub. point estimate output types (output_type_ids NA,) collected hub, output_type_id column assigned character data type auto-determined. data type values can used override automatic determination. Note attempting coerce output_type_id data type valid data (e.g. trying coerce\"character\" values \"double\") likely result error potentially unexpected behaviour use care. partitions named list specifying arrow data types partitioning column. skip_checks Logical. TRUE (default), skip validation checks opening hub datasets, providing optimal performance especially large cloud hubs (AWS S3, GCS) minimizing /O operations. However, result error model output directory contains files opened part dataset. Setting FALSE attempt open exclude invalid files read part dataset. results slower performance due increased /O operations provides robustness working directories may contain invalid files. Note hubs validated hubValidations package require additional checks. invalid (non-model output) files present model output directory, use ignore_files argument exclude . na character vector strings interpret missing values. applies CSV files. default c(\"NA\", \"\"). Useful actual character string \"NA\" values used data. case, use empty cells indicate missing values files set na = \"\". ignore_files character vector file names (paths) file prefixes ignore discovering model output files include dataset connections. Parent directory names included. Common non-data files \"README\" \".DS_Store\" ignored automatically, additional files can excluded specifying . model_output_dir Either character string path local directory containing model output data object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path directory containing model output data stored cloud. details consult Using cloud storage (S3, GCS) arrow package. partition_names character vector defines field names recursive directory names correspond . Defaults single model_id field reflects standard expected structure model-output directory. schema arrow::Schema object Dataset. NULL (default), schema inferred data sources.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to model output data. — connect_hub","text":"connect_hub returns S3 object class <hub_connection>. connect_model_output returns S3 object class <mod_out_connection>. objects connected data model-output directory via Apache arrow FileSystemDataset connection. connection can used extract data using dplyr custom queries. <hub_connection> class also contains modeling hub metadata.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Connect to model output data. — connect_hub","text":"default, common non-data files may present model output directories (e.g. \"README\", \".DS_Store\") excluded automatically prevent errors connecting via Arrow. Additional files can excluded using ignore_files parameter.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Connect to model output data. — connect_hub","text":"connect_hub(): connect fully configured Modeling Hub directory. connect_model_output(): connect directly model-output directory. function can used access data directly appropriately set model output directory part fully configured hub.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_hub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to model output data. — connect_hub","text":"","code":"# Connect to a local simple forecasting Hub. hub_path <- system.file(\"testhubs/simple\", package = \"hubUtils\") hub_con <- connect_hub(hub_path) hub_con #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/simple #> • file_format: \"csv(3/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema  #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string #> age_group: string hub_con <- connect_hub(hub_path, output_type_id_datatype = \"character\") hub_con #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/simple #> • file_format: \"csv(3/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema  #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: string #> value: int32 #> model_id: string #> age_group: string # Connect directly to a local `model-output` directory mod_out_path <- system.file(\"testhubs/simple/model-output\", package = \"hubUtils\") mod_out_con <- connect_model_output(mod_out_path) mod_out_con #>  #> ── <mod_out_connection/FileSystemDataset> ── #>  #> • file_format: \"csv(3/3)\" #> • checks: TRUE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #>  #> ── Connection schema  #> mod_out_connection with 3 csv files #> 8 columns #> origin_date: date32[day] #> target: string #> horizon: int64 #> location: string #> output_type: string #> output_type_id: double #> value: int64 #> model_id: string # Query hub_connection for data library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union hub_con %>%   filter(     origin_date == \"2022-10-08\",     horizon == 2   ) %>%   collect_hub() #> # A tibble: 69 × 9 #>    model_id     origin_date target        horizon location age_group output_type #>  * <chr>        <date>      <chr>           <int> <chr>    <chr>     <chr>       #>  1 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  2 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  3 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  4 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  5 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  6 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  7 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  8 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #>  9 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #> 10 hub-baseline 2022-10-08  wk inc flu h…       2 US       NA        quantile    #> # ℹ 59 more rows #> # ℹ 2 more variables: output_type_id <chr>, value <int> mod_out_con %>%   filter(     origin_date == \"2022-10-08\",     horizon == 2   ) %>%   collect_hub() #> # A tibble: 69 × 8 #>    model_id origin_date target horizon location output_type output_type_id value #>  * <chr>    <date>      <chr>    <int> <chr>    <chr>                <dbl> <int> #>  1 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.01    135 #>  2 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.025   137 #>  3 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.05    139 #>  4 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.1     140 #>  5 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.15    141 #>  6 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.2     141 #>  7 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.25    142 #>  8 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.3     143 #>  9 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.35    144 #> 10 hub-bas… 2022-10-08  wk in…       2 US       quantile             0.4     145 #> # ℹ 59 more rows # Ignore a file connect_hub(hub_path, ignore_files = c(\"README\", \"2022-10-08-team1-goodmodel.csv\")) #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/simple #> • file_format: \"csv(2/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema  #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string #> age_group: string # Connect to a simple forecasting Hub stored in an AWS S3 bucket. if (FALSE) { # \\dontrun{ hub_path <- s3_bucket(\"hubverse/hubutils/testhubs/simple/\") hub_con <- connect_hub(hub_path) hub_con } # }"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Open connection to oracle-output target data — connect_target_oracle_output","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"Open oracle-output target data file(s) hub arrow dataset.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"","code":"connect_target_oracle_output(   hub_path = \".\",   na = c(\"NA\", \"\"),   ignore_files = NULL,   output_type_id_datatype = c(\"from_config\", \"auto\", \"character\", \"double\", \"integer\",     \"logical\", \"Date\") )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. hub must fully configured valid admin.json tasks.json files within hub-config directory. na character vector strings interpret missing values. applies CSV files. default c(\"NA\", \"\"). Useful actual character string \"NA\" values used data. case, use empty cells indicate missing values files set na = \"\". ignore_files character vector file names (paths) file prefixes ignore discovering model output files include dataset connections. Parent directory names included. Common non-data files \"README\" \".DS_Store\" ignored automatically, additional files can excluded specifying . output_type_id_datatype character string. One \"from_config\", \"auto\", \"character\", \"double\", \"integer\", \"logical\", \"Date\". Defaults \"from_config\" uses setting output_type_id_datatype property tasks.json config file available. property set config, argument falls back \"auto\" determines  output_type_id data type automatically tasks.json config file simplest data type required represent output type ID values across output types hub. point estimate output types (output_type_ids NA,) collected hub, output_type_id column assigned character data type auto-determined. data type values can used override automatic determination. Note attempting coerce output_type_id data type valid data (e.g. trying coerce\"character\" values \"double\") likely result error potentially unexpected behaviour use care.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"arrow dataset object subclass <target_oracle_output>.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"target data split across multiple files oracle-output directory, files must share file format, either csv parquet. types files currently allowed oracle-output directory.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"schema-creation","dir":"Reference","previous_headings":"","what":"Schema Creation","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"function uses different methods create Arrow schema depending hub configuration version: v6+ hubs (target-data.json): Schema created directly target-data.json configuration file using create_oracle_output_schema(). config-based approach fast deterministic, requiring filesystem /O scan data files. especially beneficial cloud storage file scanning can slow. Hubs (without target-data.json): Schema inferred scanning actual data files. inference-based approach examines file structure content determine column types. function automatically detects method use based presence target-data.json hub configuration.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"schema-ordering","dir":"Reference","previous_headings":"","what":"Schema Ordering","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"Column ordering resulting dataset depends configuration version file format: v6+ hubs (target-data.json): Parquet: Columns reordered standard hubverse convention (see get_target_data_colnames()). Parquet's column--name matching enables safe reordering. CSV: Original file ordering preserved avoid column name/position mismatches collection. Hubs (without target-data.json): Original file ordering preserved regardless format.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_oracle_output.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open connection to oracle-output target data — connect_target_oracle_output","text":"","code":"# Column Ordering: CSV vs Parquet in v6+ hubs # For v6+ hubs with target-data.json, ordering differs by file format  # Example 1: CSV format (single file) - preserves original file ordering hub_path_csv <- system.file(\"testhubs/v6/target_file\", package = \"hubUtils\") oo_con_csv <- connect_target_oracle_output(hub_path_csv)  # CSV columns are in their original file order names(oo_con_csv) #> [1] \"location\"        \"target_end_date\" \"target\"          \"output_type\"     #> [5] \"output_type_id\"  \"oracle_value\"     # Collect and filter as usual oo_con_csv |> dplyr::collect() #> # A tibble: 627 × 6 #>    location target_end_date target       output_type output_type_id oracle_value #>    <chr>    <date>          <chr>        <chr>       <chr>                 <dbl> #>  1 US       2022-10-22      wk flu hosp… cdf         1                         1 #>  2 US       2022-10-22      wk flu hosp… cdf         2                         1 #>  3 US       2022-10-22      wk flu hosp… cdf         3                         1 #>  4 US       2022-10-22      wk flu hosp… cdf         4                         1 #>  5 US       2022-10-22      wk flu hosp… cdf         5                         1 #>  6 US       2022-10-22      wk flu hosp… cdf         6                         1 #>  7 US       2022-10-22      wk flu hosp… cdf         7                         1 #>  8 US       2022-10-22      wk flu hosp… cdf         8                         1 #>  9 US       2022-10-22      wk flu hosp… cdf         9                         1 #> 10 US       2022-10-22      wk flu hosp… cdf         10                        1 #> # ℹ 617 more rows oo_con_csv |>   dplyr::filter(location == \"US\") |>   dplyr::collect() #> # A tibble: 209 × 6 #>    location target_end_date target       output_type output_type_id oracle_value #>    <chr>    <date>          <chr>        <chr>       <chr>                 <dbl> #>  1 US       2022-10-22      wk flu hosp… cdf         1                         1 #>  2 US       2022-10-22      wk flu hosp… cdf         2                         1 #>  3 US       2022-10-22      wk flu hosp… cdf         3                         1 #>  4 US       2022-10-22      wk flu hosp… cdf         4                         1 #>  5 US       2022-10-22      wk flu hosp… cdf         5                         1 #>  6 US       2022-10-22      wk flu hosp… cdf         6                         1 #>  7 US       2022-10-22      wk flu hosp… cdf         7                         1 #>  8 US       2022-10-22      wk flu hosp… cdf         8                         1 #>  9 US       2022-10-22      wk flu hosp… cdf         9                         1 #> 10 US       2022-10-22      wk flu hosp… cdf         10                        1 #> # ℹ 199 more rows  # Example 2: Parquet format (directory) - reordered to hubverse convention hub_path_parquet <- system.file(\"testhubs/v6/target_dir\", package = \"hubUtils\") oo_con_parquet <- connect_target_oracle_output(hub_path_parquet)  # Parquet columns follow hubverse convention (date first, then alphabetical) names(oo_con_parquet) #> [1] \"target_end_date\" \"target\"          \"location\"        \"output_type\"     #> [5] \"output_type_id\"  \"oracle_value\"     # Reordering is safe for Parquet because it matches columns by name # rather than position during collection oo_con_parquet |> dplyr::collect() #> # A tibble: 627 × 6 #>    target_end_date target       location output_type output_type_id oracle_value #>    <date>          <chr>        <chr>    <chr>       <chr>                 <dbl> #>  1 2022-10-22      wk flu hosp… US       cdf         1                         1 #>  2 2022-10-22      wk flu hosp… US       cdf         2                         1 #>  3 2022-10-22      wk flu hosp… US       cdf         3                         1 #>  4 2022-10-22      wk flu hosp… US       cdf         4                         1 #>  5 2022-10-22      wk flu hosp… US       cdf         5                         1 #>  6 2022-10-22      wk flu hosp… US       cdf         6                         1 #>  7 2022-10-22      wk flu hosp… US       cdf         7                         1 #>  8 2022-10-22      wk flu hosp… US       cdf         8                         1 #>  9 2022-10-22      wk flu hosp… US       cdf         9                         1 #> 10 2022-10-22      wk flu hosp… US       cdf         10                        1 #> # ℹ 617 more rows  # Both formats support the same filtering operations oo_con_parquet |>   dplyr::filter(target_end_date ==  \"2022-12-31\") |>   dplyr::collect() #> # A tibble: 57 × 6 #>    target_end_date target       location output_type output_type_id oracle_value #>    <date>          <chr>        <chr>    <chr>       <chr>                 <dbl> #>  1 2022-12-31      wk flu hosp… US       cdf         1                         0 #>  2 2022-12-31      wk flu hosp… US       cdf         2                         0 #>  3 2022-12-31      wk flu hosp… US       cdf         3                         0 #>  4 2022-12-31      wk flu hosp… US       cdf         4                         0 #>  5 2022-12-31      wk flu hosp… US       cdf         5                         0 #>  6 2022-12-31      wk flu hosp… US       cdf         6                         1 #>  7 2022-12-31      wk flu hosp… US       cdf         7                         1 #>  8 2022-12-31      wk flu hosp… US       cdf         8                         1 #>  9 2022-12-31      wk flu hosp… US       cdf         9                         1 #> 10 2022-12-31      wk flu hosp… US       cdf         10                        1 #> # ℹ 47 more rows  # Get distinct target_end_date values oo_con_parquet |>   dplyr::distinct(target_end_date) |>   dplyr::pull(as_vector = TRUE) #>  [1] \"2022-10-22\" \"2022-10-29\" \"2022-11-05\" \"2022-11-12\" \"2022-11-19\" #>  [6] \"2022-11-26\" \"2022-12-03\" \"2022-12-10\" \"2022-12-17\" \"2022-12-24\" #> [11] \"2022-12-31\"  if (FALSE) { # \\dontrun{ # Access Target oracle-output data from a cloud hub s3_hub_path <- s3_bucket(\"example-complex-forecast-hub\") s3_con <- connect_target_oracle_output(s3_hub_path) s3_con s3_con |> dplyr::collect() } # }"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":null,"dir":"Reference","previous_headings":"","what":"Open connection to time-series target data — connect_target_timeseries","title":"Open connection to time-series target data — connect_target_timeseries","text":"Open time-series target data file(s) hub arrow dataset.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open connection to time-series target data — connect_target_timeseries","text":"","code":"connect_target_timeseries(   hub_path = \".\",   date_col = NULL,   na = c(\"NA\", \"\"),   ignore_files = NULL )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open connection to time-series target data — connect_target_timeseries","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. hub must fully configured valid admin.json tasks.json files within hub-config directory. date_col Optional column name interpreted date. Default NULL. Useful required date column partitioning column target data name date typed task ID variable config. na character vector strings interpret missing values. applies CSV files. default c(\"NA\", \"\"). Useful actual character string \"NA\" values used data. case, use empty cells indicate missing values files set na = \"\". ignore_files character vector file names (paths) file prefixes ignore discovering model output files include dataset connections. Parent directory names included. Common non-data files \"README\" \".DS_Store\" ignored automatically, additional files can excluded specifying .","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open connection to time-series target data — connect_target_timeseries","text":"arrow dataset object subclass <target_timeseries>.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Open connection to time-series target data — connect_target_timeseries","text":"target data split across multiple files time-series directory, files must share file format, either csv parquet. types files currently allowed time-series directory.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"schema-creation","dir":"Reference","previous_headings":"","what":"Schema Creation","title":"Open connection to time-series target data — connect_target_timeseries","text":"function uses different methods create Arrow schema depending hub configuration version: v6+ hubs (target-data.json): Schema created directly target-data.json configuration file using create_timeseries_schema(). config-based approach fast deterministic, requiring filesystem /O scan data files. especially beneficial cloud storage file scanning can slow. Hubs (without target-data.json): Schema inferred scanning actual data files. inference-based approach examines file structure content determine column types. function automatically detects method use based presence target-data.json hub configuration.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"schema-ordering","dir":"Reference","previous_headings":"","what":"Schema Ordering","title":"Open connection to time-series target data — connect_target_timeseries","text":"Column ordering resulting dataset depends configuration version file format: v6+ hubs (target-data.json): Parquet: Columns reordered standard hubverse convention (see get_target_data_colnames()). Parquet's column--name matching enables safe reordering. CSV: Original file ordering preserved avoid column name/position mismatches collection. Hubs (without target-data.json): Original file ordering preserved regardless format.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/connect_target_timeseries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open connection to time-series target data — connect_target_timeseries","text":"","code":"# Column Ordering: CSV vs Parquet in v6+ hubs # For v6+ hubs with target-data.json, ordering differs by file format  # Example 1: CSV format (single file) - preserves original file ordering hub_path_csv <- system.file(\"testhubs/v6/target_file\", package = \"hubUtils\") ts_con_csv <- connect_target_timeseries(hub_path_csv)  # CSV columns are in their original file order names(ts_con_csv) #> [1] \"target_end_date\" \"target\"          \"location\"        \"observation\"     # Note: columns appear in the order they are in the CSV file  # Collect and filter as usual ts_con_csv |> dplyr::collect() #> # A tibble: 66 × 4 #>    target_end_date target          location observation #>    <date>          <chr>           <chr>          <dbl> #>  1 2022-10-22      wk inc flu hosp 02                 3 #>  2 2022-10-22      wk inc flu hosp 01               141 #>  3 2022-10-22      wk inc flu hosp US              2380 #>  4 2022-10-29      wk inc flu hosp 02                14 #>  5 2022-10-29      wk inc flu hosp 01               262 #>  6 2022-10-29      wk inc flu hosp US              4353 #>  7 2022-11-05      wk inc flu hosp 02                10 #>  8 2022-11-05      wk inc flu hosp 01               360 #>  9 2022-11-05      wk inc flu hosp US              6571 #> 10 2022-11-12      wk inc flu hosp 02                20 #> # ℹ 56 more rows ts_con_csv |>   dplyr::filter(location == \"US\") |>   dplyr::collect() #> # A tibble: 22 × 4 #>    target_end_date target          location observation #>    <date>          <chr>           <chr>          <dbl> #>  1 2022-10-22      wk inc flu hosp US              2380 #>  2 2022-10-29      wk inc flu hosp US              4353 #>  3 2022-11-05      wk inc flu hosp US              6571 #>  4 2022-11-12      wk inc flu hosp US              8848 #>  5 2022-11-19      wk inc flu hosp US             11427 #>  6 2022-11-26      wk inc flu hosp US             19846 #>  7 2022-12-03      wk inc flu hosp US             26333 #>  8 2022-12-10      wk inc flu hosp US             23851 #>  9 2022-12-17      wk inc flu hosp US             21435 #> 10 2022-12-24      wk inc flu hosp US             19286 #> # ℹ 12 more rows  # Example 2: Parquet format (directory) - reordered to hubverse convention hub_path_parquet <- system.file(\"testhubs/v6/target_dir\", package = \"hubUtils\") ts_con_parquet <- connect_target_timeseries(hub_path_parquet)  # Parquet columns follow hubverse convention names(ts_con_parquet) #> [1] \"target_end_date\" \"target\"          \"location\"        \"observation\"      # Reordering is safe for Parquet because it matches columns by name # rather than position during collection ts_con_parquet |> dplyr::collect() #> # A tibble: 66 × 4 #>    target_end_date target           location observation #>    <date>          <chr>            <chr>          <dbl> #>  1 2022-10-22      wk flu hosp rate 02             0.422 #>  2 2022-10-22      wk flu hosp rate 01             2.78  #>  3 2022-10-22      wk flu hosp rate US             0.716 #>  4 2022-10-29      wk flu hosp rate 02             1.97  #>  5 2022-10-29      wk flu hosp rate 01             5.17  #>  6 2022-10-29      wk flu hosp rate US             1.31  #>  7 2022-11-05      wk flu hosp rate 02             1.41  #>  8 2022-11-05      wk flu hosp rate 01             7.11  #>  9 2022-11-05      wk flu hosp rate US             1.98  #> 10 2022-11-12      wk flu hosp rate 02             2.81  #> # ℹ 56 more rows  # Both formats support the same filtering operations ts_con_parquet |>   dplyr::filter(target_end_date ==  \"2022-12-31\") |>   dplyr::collect() #> # A tibble: 6 × 4 #>   target_end_date target           location observation #>   <date>          <chr>            <chr>          <dbl> #> 1 2022-12-31      wk flu hosp rate 02              6.18 #> 2 2022-12-31      wk flu hosp rate 01              2.76 #> 3 2022-12-31      wk flu hosp rate US              5.83 #> 4 2022-12-31      wk inc flu hosp  02             44    #> 5 2022-12-31      wk inc flu hosp  01            140    #> 6 2022-12-31      wk inc flu hosp  US          19369     if (FALSE) { # \\dontrun{ # Access Target time-series data from a cloud hub s3_hub_path <- s3_bucket(\"example-complex-forecast-hub\") s3_con <- connect_target_timeseries(s3_hub_path) s3_con s3_con |> dplyr::collect() } # }"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_hub_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Hub arrow schema — create_hub_schema","title":"Create a Hub arrow schema — create_hub_schema","text":"Create arrow schema tasks.json config file. use opening arrow dataset.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_hub_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Hub arrow schema — create_hub_schema","text":"","code":"create_hub_schema(   config_tasks,   partitions = list(model_id = arrow::utf8()),   output_type_id_datatype = c(\"from_config\", \"auto\", \"character\", \"double\", \"integer\",     \"logical\", \"Date\"),   r_schema = FALSE )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_hub_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Hub arrow schema — create_hub_schema","text":"config_tasks list version content's hub's tasks.json config file created using function hubUtils::read_config(). partitions named list specifying arrow data types partitioning column. output_type_id_datatype character string. One \"from_config\", \"auto\", \"character\", \"double\", \"integer\", \"logical\", \"Date\". Defaults \"from_config\" uses setting output_type_id_datatype property tasks.json config file available. property set config, argument falls back \"auto\" determines  output_type_id data type automatically tasks.json config file simplest data type required represent output type ID values across output types hub. point estimate output types (output_type_ids NA,) collected hub, output_type_id column assigned character data type auto-determined. data type values can used override automatic determination. Note attempting coerce output_type_id data type valid data (e.g. trying coerce\"character\" values \"double\") likely result error potentially unexpected behaviour use care. r_schema Logical. FALSE (default), return arrow::schema() object. TRUE, return character vector R data types.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_hub_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Hub arrow schema — create_hub_schema","text":"arrow schema object can used define column datatypes opening model output data. r_schema = TRUE, character vector R data types.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_hub_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Hub arrow schema — create_hub_schema","text":"","code":"hub_path <- system.file(\"testhubs/simple\", package = \"hubUtils\") config_tasks <- hubUtils::read_config(hub_path, \"tasks\") schema <- create_hub_schema(config_tasks)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_model_out_submit_tmpl.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a model output submission file template — create_model_out_submit_tmpl","title":"Create a model output submission file template — create_model_out_submit_tmpl","text":"function moved hubValidations package renamed submission_tmpl().","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_model_out_submit_tmpl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a model output submission file template — create_model_out_submit_tmpl","text":"","code":"create_model_out_submit_tmpl(   hub_con,   config_tasks,   round_id,   required_vals_only = FALSE,   complete_cases_only = TRUE )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_model_out_submit_tmpl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a model output submission file template — create_model_out_submit_tmpl","text":"hub_con ⁠<hub_connection>⁠ class object. config_tasks list version content's hub's tasks.json config file, accessed \"config_tasks\" attribute <hub_connection> object function hubUtils::read_config(). round_id Character string. Round identifier. round set round_id_from_variable: true, IDs values task ID defined round's round_id property config_tasks. Otherwise match round's round_id value config. Ignored hub contains single round. required_vals_only Logical. Whether return combinations Task ID related output type ID required values. complete_cases_only Logical. TRUE (default) required_vals_only = TRUE, rows complete cases combinations required values returned. FALSE, rows incomplete cases combinations required values included output.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_model_out_submit_tmpl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a model output submission file template — create_model_out_submit_tmpl","text":"tibble template containing expanded grid valid task ID output type ID value combinations given submission round output type. required_vals_only = TRUE, values limited combination required values .","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_model_out_submit_tmpl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a model output submission file template — create_model_out_submit_tmpl","text":"task IDs output_type_ids values optional, default, columns included columns NAs required_vals_only = TRUE. columns exist, function returns tibble zero rows, complete cases required value combinations exists. (Note determination complete cases excludes valid NA output_type_id values \"mean\" \"median\" output types). return template incomplete required cases, includes NA columns, use complete_cases_only = FALSE. sample output types included output, output_type_id column contains example sample indexes useful identifying compound task ID structure multivariate sampling distributions particular, .e. combinations task ID values represent individual samples. round set round_id_from_variable: true, value task ID round IDs derived (.e. task ID specified round_id property config_tasks) set value round_id argument returned output.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_oracle_output_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create oracle-output target data file schema — create_oracle_output_schema","title":"Create oracle-output target data file schema — create_oracle_output_schema","text":"Create oracle-output target data file schema","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_oracle_output_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create oracle-output target data file schema — create_oracle_output_schema","text":"","code":"create_oracle_output_schema(   hub_path,   na = c(\"NA\", \"\"),   ignore_files = NULL,   r_schema = FALSE,   output_type_id_datatype = c(\"from_config\", \"auto\", \"character\", \"double\", \"integer\",     \"logical\", \"Date\") )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_oracle_output_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create oracle-output target data file schema — create_oracle_output_schema","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. hub must fully configured valid admin.json tasks.json files within hub-config directory. na character vector strings interpret missing values. applies CSV files. default c(\"NA\", \"\"). Useful actual character string \"NA\" values used data. case, use empty cells indicate missing values files set na = \"\". ignore_files character vector file names (paths) file prefixes ignore discovering model output files include dataset connections. Parent directory names included. Common non-data files \"README\" \".DS_Store\" ignored automatically, additional files can excluded specifying . r_schema Logical. FALSE (default), return arrow::schema() object. TRUE, return character vector R data types. output_type_id_datatype character string. One \"from_config\", \"auto\", \"character\", \"double\", \"integer\", \"logical\", \"Date\". Defaults \"from_config\" uses setting output_type_id_datatype property tasks.json config file available. property set config, argument falls back \"auto\" determines  output_type_id data type automatically tasks.json config file simplest data type required represent output type ID values across output types hub. point estimate output types (output_type_ids NA,) collected hub, output_type_id column assigned character data type auto-determined. data type values can used override automatic determination. Note attempting coerce output_type_id data type valid data (e.g. trying coerce\"character\" values \"double\") likely result error potentially unexpected behaviour use care.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_oracle_output_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create oracle-output target data file schema — create_oracle_output_schema","text":"arrow <schema> class object","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_oracle_output_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create oracle-output target data file schema — create_oracle_output_schema","text":"target-data.json (v6.0.0+) present, schema created directly config without reading target data files. Otherwise, schema inferred reading dataset. Config-based approach avoids file /O (especially beneficial cloud storage) provides deterministic schema creation.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_oracle_output_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create oracle-output target data file schema — create_oracle_output_schema","text":"","code":"hub_path <- system.file(\"testhubs/v5/target_file\", package = \"hubUtils\") # Create target oracle-output schema create_oracle_output_schema(hub_path) #> Schema #> location: string #> target_end_date: date32[day] #> target: string #> output_type: string #> output_type_id: string #> oracle_value: double #  target oracle-output schema from a cloud hub s3_hub_path <- s3_bucket(\"example-complex-forecast-hub\") create_oracle_output_schema(s3_hub_path) #> Schema #> target_end_date: date32[day] #> target: string #> location: string #> output_type: string #> output_type_id: string #> oracle_value: double"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_timeseries_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Create time-series target data file schema — create_timeseries_schema","title":"Create time-series target data file schema — create_timeseries_schema","text":"Create time-series target data file schema","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_timeseries_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create time-series target data file schema — create_timeseries_schema","text":"","code":"create_timeseries_schema(   hub_path,   date_col = NULL,   na = c(\"NA\", \"\"),   ignore_files = NULL,   r_schema = FALSE )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_timeseries_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create time-series target data file schema — create_timeseries_schema","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. hub must fully configured valid admin.json tasks.json files within hub-config directory. date_col Optional column name interpreted date. Default NULL. Useful required date column partitioning column target data name date typed task ID variable config. Note: Ignored target-data.json exists (v6+); date column read config. na character vector strings interpret missing values. applies CSV files. default c(\"NA\", \"\"). Useful actual character string \"NA\" values used data. case, use empty cells indicate missing values files set na = \"\". ignore_files character vector file names (paths) file prefixes ignore discovering model output files include dataset connections. Parent directory names included. Common non-data files \"README\" \".DS_Store\" ignored automatically, additional files can excluded specifying . r_schema Logical. FALSE (default), return arrow::schema() object. TRUE, return character vector R data types.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_timeseries_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create time-series target data file schema — create_timeseries_schema","text":"arrow <schema> class object","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_timeseries_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create time-series target data file schema — create_timeseries_schema","text":"target-data.json (v6.0.0+) present, schema created directly config without reading target data files. Otherwise, schema inferred reading dataset. Config-based approach avoids file /O (especially beneficial cloud storage) provides deterministic schema creation.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/create_timeseries_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create time-series target data file schema — create_timeseries_schema","text":"","code":"hub_path <- system.file(\"testhubs/v5/target_file\", package = \"hubUtils\") # Create target time-series schema create_timeseries_schema(hub_path) #> Schema #> target_end_date: date32[day] #> target: string #> location: string #> observation: double #  target time-series schema from a cloud hub s3_hub_path <- s3_bucket(\"example-complex-forecast-hub\") create_timeseries_schema(s3_hub_path) #> Schema #> target_end_date: date32[day] #> target: string #> location: string #> observation: double"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/expand_model_out_val_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Create expanded grid of valid task ID and output type value combinations — expand_model_out_val_grid","title":"Create expanded grid of valid task ID and output type value combinations — expand_model_out_val_grid","text":"function moved hubValidations package renamed expand_model_out_grid().","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/expand_model_out_val_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create expanded grid of valid task ID and output type value combinations — expand_model_out_val_grid","text":"","code":"expand_model_out_val_grid(   config_tasks,   round_id,   required_vals_only = FALSE,   all_character = FALSE,   as_arrow_table = FALSE,   bind_model_tasks = TRUE,   include_sample_ids = FALSE )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/expand_model_out_val_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create expanded grid of valid task ID and output type value combinations — expand_model_out_val_grid","text":"config_tasks list version content's hub's tasks.json config file, accessed \"config_tasks\" attribute <hub_connection> object function hubUtils::read_config(). round_id Character string. Round identifier. round set round_id_from_variable: true, IDs values task ID defined round's round_id property config_tasks. Otherwise match round's round_id value config. Ignored hub contains single round. required_vals_only Logical. Whether return combinations Task ID related output type ID required values. all_character Logical. Whether return character column. as_arrow_table Logical. Whether return arrow table. Defaults FALSE. bind_model_tasks Logical. Whether bind expanded grids values multiple modeling tasks single tibble/arrow table return list. include_sample_ids Logical. Whether include sample identifiers output_type_id column.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/expand_model_out_val_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create expanded grid of valid task ID and output type value combinations — expand_model_out_val_grid","text":"bind_model_tasks = TRUE (default) tibble arrow table containing possible task ID related output type ID value combinations. bind_model_tasks = FALSE, list containing tibble arrow table round modeling task. Columns coerced data types according hub schema, unless all_character = TRUE. all_character = TRUE, columns returned character can faster large expanded grids expected. required_vals_only = TRUE, values limited combinations required values .","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/expand_model_out_val_grid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create expanded grid of valid task ID and output type value combinations — expand_model_out_val_grid","text":"round set round_id_from_variable: true, value task ID round IDs derived (.e. task ID specified round_id property config_tasks) set value round_id argument returned output. sample output types included output include_sample_ids = TRUE, output_type_id column contains example sample indexes useful identifying compound task ID structure multivariate sampling distributions particular, .e. combinations task ID values represent individual samples.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/extract_hive_partitions.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","title":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","text":"Given filesystem path, function extracts Hive-style partition key-value pairs (.e., path components formatted key=value). supports decoding URL-encoded values (e.g., \"wk%20flu\" → \"wk flu\"), handles empty values (e.g., \"key=\") NA, consistent Hive Arrow semantics.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/extract_hive_partitions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","text":"","code":"extract_hive_partitions(path, strict = TRUE)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/extract_hive_partitions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","text":"path character string length 1: path file directory. strict Logical. TRUE, invalid partition segments (e.g., =value, just =) trigger error. FALSE, valid key=value components returned.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/extract_hive_partitions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","text":"named character vector names partition keys values decoded values. Returns NULL valid partitions found.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/extract_hive_partitions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","text":"strict = TRUE, function abort detailed error message malformed partition-like segments found.","code":""},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/reference/extract_hive_partitions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Hive-style partition key-value pairs from a path — extract_hive_partitions","text":"","code":"extract_hive_partitions(\"data/country=US/year=2024/file.parquet\") #> country    year  #>    \"US\"  \"2024\"  extract_hive_partitions(\"data/country=/year=2024/\", strict = TRUE) #> country    year  #>      NA  \"2024\"  # extract_hive_partitions(\"data/=US/year=2024/\", strict = TRUE) # This will error"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_s3_bucket_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the bucket name for the cloud storage location. — get_s3_bucket_name","title":"Get the bucket name for the cloud storage location. — get_s3_bucket_name","text":"Get bucket name cloud storage location.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_s3_bucket_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the bucket name for the cloud storage location. — get_s3_bucket_name","text":"","code":"get_s3_bucket_name(hub_path = \".\")"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_s3_bucket_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the bucket name for the cloud storage location. — get_s3_bucket_name","text":"hub_path Path hub directory.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_s3_bucket_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the bucket name for the cloud storage location. — get_s3_bucket_name","text":"bucket name cloud storage location.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_s3_bucket_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the bucket name for the cloud storage location. — get_s3_bucket_name","text":"","code":"hub_path <- system.file(\"testhubs/v5/target_file\", package = \"hubUtils\") get_s3_bucket_name(hub_path) #> [1] \"example-complex-forecast-hub\" # Get config info from GitHub get_s3_bucket_name(   \"https://github.com/hubverse-org/example-complex-forecast-hub\" ) #> [1] \"example-complex-forecast-hub\""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_data_colnames.html","id":null,"dir":"Reference","previous_headings":"","what":"Get expected target data column names from config — get_target_data_colnames","title":"Get expected target data column names from config — get_target_data_colnames","text":"Extracts expected column names target data hub's configuration files correct order. useful validation schema generation without needing inspect actual dataset.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_data_colnames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get expected target data column names from config — get_target_data_colnames","text":"","code":"get_target_data_colnames(   config_target_data,   target_type = c(\"time-series\", \"oracle-output\") )"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_data_colnames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get expected target data column names from config — get_target_data_colnames","text":"config_target_data config_target_data object (hubUtils::read_config(hub_path, \"target-data\")) target_type Character string specifying target data type. Must either \"time-series\" \"oracle-output\".","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_data_colnames.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get expected target data column names from config — get_target_data_colnames","text":"character vector expected column names correct order: Date column Task ID columns (observable_unit) Non-task ID columns (time-series , specified config) Output type columns (output_type output_type_id, oracle-output specified config) Target value column (observation time-series, oracle_value oracle-output) as_of column (data versioned)","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_data_colnames.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get expected target data column names from config — get_target_data_colnames","text":"function builds column name vector directly configuration objects without requiring dataset inspection. makes lightweight, efficient, suitable validation purposes. time-series data, columns ordered : Task ID columns observable_unit Date column (observable_unit) Non-task ID columns target-data.json (present) observation column (target value) as_of column (versioned = TRUE) oracle-output data, columns ordered : Task ID columns observable_unit Date column (observable_unit) output_type output_type_id columns (has_output_type_ids = TRUE) oracle_value column (target value) as_of column (versioned = TRUE)","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_data_colnames.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get expected target data column names from config — get_target_data_colnames","text":"","code":"# Note: These examples require test data hub_path <- system.file(\"testhubs/v6/target_file\", package = \"hubUtils\") config_target_data <- hubUtils::read_config(hub_path, \"target-data\")  # Get time-series column names get_target_data_colnames(   config_target_data,   target_type = \"time-series\" ) #> [1] \"target_end_date\" \"target\"          \"location\"        \"observation\"      # Get oracle-output column names get_target_data_colnames(   config_target_data,   target_type = \"oracle-output\" ) #> [1] \"target_end_date\" \"target\"          \"location\"        \"output_type\"     #> [5] \"output_type_id\"  \"oracle_value\""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_file_ext.html","id":null,"dir":"Reference","previous_headings":"","what":"Get target data file unique file extensions. — get_target_file_ext","title":"Get target data file unique file extensions. — get_target_file_ext","text":"Get unique file extension(s) target data file(s) target_path. target_path directory, function return unique file extensions files directory. target_path file, function return file extension file.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_file_ext.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get target data file unique file extensions. — get_target_file_ext","text":"","code":"get_target_file_ext(hub_path = NULL, target_path)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_file_ext.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get target data file unique file extensions. — get_target_file_ext","text":"hub_path NULL, must SubTreeFileSystem class object root cloud hosted hub. Required trigger SubTreeFileSystem method. target_path character string. path target data file directory. Usually output get_target_path().","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_file_ext.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get target data file unique file extensions. — get_target_file_ext","text":"","code":"hub_path <- system.file(\"testhubs/v5/target_file\", package = \"hubUtils\") target_path <- get_target_path(hub_path, \"time-series\") get_target_file_ext(hub_path, target_path) #> [1] \"csv\""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the path(s) to the target data file(s) in the hub directory. — get_target_path","title":"Get the path(s) to the target data file(s) in the hub directory. — get_target_path","text":"Get path(s) target data file(s) hub directory.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the path(s) to the target data file(s) in the hub directory. — get_target_path","text":"","code":"get_target_path(hub_path, target_type = c(\"time-series\", \"oracle-output\"))"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the path(s) to the target data file(s) in the hub directory. — get_target_path","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. hub must fully configured valid admin.json tasks.json files within hub-config directory. target_type Type target data retrieve matching files. One \"time-series\" \"oracle-output\". Defaults \"time-series\".","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the path(s) to the target data file(s) in the hub directory. — get_target_path","text":"character vector path(s) target data file(s) (target-data directory) make target_type requested.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/get_target_path.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the path(s) to the target data file(s) in the hub directory. — get_target_path","text":"","code":"hub_path <- system.file(\"testhubs/v5/target_file\", package = \"hubUtils\") get_target_path(hub_path) #> /home/runner/work/_temp/Library/hubUtils/testhubs/v5/target_file/target-data/time-series.csv get_target_path(hub_path, \"time-series\") #> /home/runner/work/_temp/Library/hubUtils/testhubs/v5/target_file/target-data/time-series.csv get_target_path(hub_path, \"oracle-output\") #> /home/runner/work/_temp/Library/hubUtils/testhubs/v5/target_file/target-data/oracle-output.csv # Access cloud data s3_bucket_name <- get_s3_bucket_name(hub_path) s3_hub_path <- s3_bucket(s3_bucket_name) get_target_path(s3_hub_path) #> target-data/time-series.csv get_target_path(s3_hub_path, \"oracle-output\") #> target-data/oracle-output.csv"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/gs_bucket.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to a Google Cloud Storage (GCS) bucket — gs_bucket","title":"Connect to a Google Cloud Storage (GCS) bucket — gs_bucket","text":"See arrow::gs_bucket() details.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/gs_bucket.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to a Google Cloud Storage (GCS) bucket — gs_bucket","text":"SubTreeFileSystem containing GcsFileSystem bucket's relative path. Note function's success guarantee authorized access bucket's contents.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/gs_bucket.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to a Google Cloud Storage (GCS) bucket — gs_bucket","text":"","code":"if (FALSE) { bucket <- gs_bucket(\"voltrondata-labs-datasets\") }"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/hubData-package.html","id":null,"dir":"Reference","previous_headings":"","what":"hubData: Tools for accessing and working with hubverse data — hubData-package","title":"hubData: Tools for accessing and working with hubverse data — hubData-package","text":"set utility functions accessing working forecast target data Infectious Disease Modeling Hubs.","code":""},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/reference/hubData-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"hubData: Tools for accessing and working with hubverse data — hubData-package","text":"Maintainer: Anna Krystalli annakrystalli@googlemail.com (ORCID) contributors: Li Shandross lshandross@umass.edu [contributor] Nicholas G. Reich nick@umass.edu (ORCID) [contributor] Evan L. Ray elray@umass.edu [contributor] Becky Sweger bsweger@gmail.com [contributor] Consortium Infectious Disease Modeling Hubs [copyright holder]","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/is_hive_partitioned_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","title":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","text":"function checks given file directory path includes one Hive-style partition segments (.e., subdirectories formatted key=value). function can operate strict lenient mode, depending whether want catch malformed partition-like segments.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/is_hive_partitioned_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","text":"","code":"is_hive_partitioned_path(path, strict = TRUE)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/is_hive_partitioned_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","text":"path Character string. Path file directory. strict Logical. TRUE, function throw error malformed partition segments found (e.g., =value, missing key, malformed = without value). FALSE, simply returns TRUE valid key=value segments found.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/is_hive_partitioned_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","text":"logical value: TRUE path contains one valid Hive-style partition segments, FALSE otherwise.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/is_hive_partitioned_path.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","text":"valid partition segment must: Contain equals sign (=) non-empty key equals sign May empty value (interpreted NA Hive/Arrow contexts) strict mode, function validates key=value segments well-formed abort .","code":""},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/reference/is_hive_partitioned_path.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check whether a path contains Hive-style partitioning — is_hive_partitioned_path","text":"","code":"is_hive_partitioned_path(\"data/country=US/year=2024/file.parquet\") #> [1] TRUE is_hive_partitioned_path(\"data/country=/year=2024/\", strict = TRUE) #> [1] TRUE # is_hive_partitioned_path(\"data/=US/year=2024/\", strict = TRUE) # This will error"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/load_model_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Compile hub model metadata — load_model_metadata","title":"Compile hub model metadata — load_model_metadata","text":"Loads hub model metadata models specified subset models compiles tibble one row per model.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/load_model_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compile hub model metadata — load_model_metadata","text":"","code":"load_model_metadata(hub_path, model_ids = NULL)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/load_model_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compile hub model metadata — load_model_metadata","text":"hub_path Either character string path local Modeling Hub directory object class <SubTreeFileSystem> created using functions s3_bucket() gs_bucket() providing string S3 GCS bucket name path Modeling Hub directory stored cloud. details consult Using cloud storage (S3, GCS) arrow package. model_ids vector character strings models load metadata. Defaults NULL, case metadata models loaded.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/load_model_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compile hub model metadata — load_model_metadata","text":"tibble model metadata. One row model, one column top-level field metadata file. metadata files nested structures, tibble may contain list-columns entries lists containing nested metadata values.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/load_model_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compile hub model metadata — load_model_metadata","text":"","code":"# Load in model metadata from local hub hub_path <- system.file(\"testhubs/simple\", package = \"hubUtils\") load_model_metadata(hub_path) #> # A tibble: 2 × 15 #>   model_id        team_abbr model_abbr team_name        model_name model_version #>   <chr>           <chr>     <chr>      <chr>            <chr>      <chr>         #> 1 hub-baseline    hub       baseline   Hub Coordinatio… Baseline   1.0           #> 2 team1-goodmodel team1     goodmodel  Team1            Good Model 1.0           #> # ℹ 9 more variables: model_contributors <list>, website_url <chr>, #> #   repo_url <lgl>, license <chr>, include_viz <lgl>, include_ensemble <lgl>, #> #   include_eval <lgl>, model_details <list>, ensemble_of_hub_models <lgl> load_model_metadata(hub_path, model_ids = c(\"hub-baseline\")) #> # A tibble: 1 × 15 #>   model_id     team_abbr model_abbr team_name           model_name model_version #>   <chr>        <chr>     <chr>      <chr>               <chr>      <chr>         #> 1 hub-baseline hub       baseline   Hub Coordination T… Baseline   1.0           #> # ℹ 9 more variables: model_contributors <list>, website_url <chr>, #> #   repo_url <lgl>, license <chr>, include_viz <lgl>, include_ensemble <lgl>, #> #   include_eval <lgl>, model_details <list>, ensemble_of_hub_models <lgl>"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/print.hub_connection.html","id":null,"dir":"Reference","previous_headings":"","what":"Print a <hub_connection> or <mod_out_connection> S3 class object — print.hub_connection","title":"Print a <hub_connection> or <mod_out_connection> S3 class object — print.hub_connection","text":"Print <hub_connection> <mod_out_connection> S3 class object","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/print.hub_connection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print a <hub_connection> or <mod_out_connection> S3 class object — print.hub_connection","text":"","code":"# S3 method for class 'hub_connection' print(x, verbose = FALSE, ...)  # S3 method for class 'mod_out_connection' print(x, verbose = FALSE, ...)"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/print.hub_connection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print a <hub_connection> or <mod_out_connection> S3 class object — print.hub_connection","text":"x <hub_connection> <mod_out_connection> S3 class object. verbose Logical. Whether print full structure object. Defaults FALSE. ... arguments passed methods.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/print.hub_connection.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Print a <hub_connection> or <mod_out_connection> S3 class object — print.hub_connection","text":"print(hub_connection): print <hub_connection> object. print(mod_out_connection): print <mod_out_connection> object.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/print.hub_connection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print a <hub_connection> or <mod_out_connection> S3 class object — print.hub_connection","text":"","code":"hub_path <- system.file(\"testhubs/simple\", package = \"hubUtils\") hub_con <- connect_hub(hub_path) hub_con #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/simple #> • file_format: \"csv(3/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema  #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string #> age_group: string print(hub_con) #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/simple #> • file_format: \"csv(3/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema  #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string #> age_group: string print(hub_con, verbose = TRUE) #>  #> ── <hub_connection/UnionDataset> ── #>  #> • hub_name: \"Simple Forecast Hub\" #> • hub_path: /home/runner/work/_temp/Library/hubUtils/testhubs/simple #> • file_format: \"csv(3/3)\" and \"parquet(1/1)\" #> • checks: FALSE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #> • config_admin: hub-config/admin.json #> • config_tasks: hub-config/tasks.json #>  #> ── Connection schema  #> hub_connection #> 9 columns #> origin_date: date32[day] #> target: string #> horizon: int32 #> location: string #> output_type: string #> output_type_id: double #> value: int32 #> model_id: string #> age_group: string #> Classes 'hub_connection', 'UnionDataset', 'Dataset', 'ArrowObject', 'R6' <hub_connection> #>   Inherits from: <UnionDataset> #>   Public: #>     .:xp:.: externalptr #>     .unsafe_delete: function ()  #>     NewScan: function ()  #>     ToString: function ()  #>     WithSchema: function (schema)  #>     children: active binding #>     class_title: function ()  #>     clone: function (deep = FALSE)  #>     initialize: function (xp)  #>     metadata: active binding #>     num_cols: active binding #>     num_rows: active binding #>     pointer: function ()  #>     print: function (...)  #>     schema: active binding #>     set_pointer: function (xp)  #>     type: active binding  #>  - attr(*, \"hub_name\")= chr \"Simple Forecast Hub\" #>  - attr(*, \"file_format\")= int [1:2, 1:2] 3 3 1 1 #>   ..- attr(*, \"dimnames\")=List of 2 #>   .. ..$ : chr [1:2] \"n_open\" \"n_in_dir\" #>   .. ..$ : chr [1:2] \"csv\" \"parquet\" #>  - attr(*, \"checks\")= logi FALSE #>  - attr(*, \"file_system\")= chr \"LocalFileSystem\" #>  - attr(*, \"hub_path\")= chr \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple\" #>  - attr(*, \"model_output_dir\")= chr \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #>  - attr(*, \"config_admin\")=List of 8 #>   ..$ schema_version: chr \"https://raw.githubusercontent.com/hubverse-org/schemas/main/v2.0.0/admin-schema.json\" #>   ..$ name          : chr \"Simple Forecast Hub\" #>   ..$ maintainer    : chr \"Consortium of Infectious Disease Modeling Hubs\" #>   ..$ contact       :List of 2 #>   .. ..$ name : chr \"Joe Bloggs\" #>   .. ..$ email: chr \"j.bloggs@cidmh.com\" #>   ..$ repository_url: chr \"https://github.com/hubverse-org/example-simple-forecast-hub\" #>   ..$ hub_models    :List of 1 #>   .. ..$ :List of 3 #>   .. .. ..$ team_abbr : chr \"simple_hub\" #>   .. .. ..$ model_abbr: chr \"baseline\" #>   .. .. ..$ model_type: chr \"baseline\" #>   ..$ file_format   : chr [1:3] \"csv\" \"parquet\" \"arrow\" #>   ..$ timezone      : chr \"US/Eastern\" #>   ..- attr(*, \"schema_id\")= chr \"https://raw.githubusercontent.com/hubverse-org/schemas/main/v2.0.0/admin-schema.json\" #>   ..- attr(*, \"type\")= chr \"admin\" #>   ..- attr(*, \"class\")= chr [1:2] \"config\" \"list\" #>  - attr(*, \"config_tasks\")=List of 2 #>   ..$ schema_version: chr \"https://raw.githubusercontent.com/hubverse-org/schemas/main/v2.0.0/tasks-schema.json\" #>   ..$ rounds        :List of 2 #>   .. ..$ :List of 4 #>   .. .. ..$ round_id_from_variable: logi TRUE #>   .. .. ..$ round_id              : chr \"origin_date\" #>   .. .. ..$ model_tasks           :List of 1 #>   .. .. .. ..$ :List of 3 #>   .. .. .. .. ..$ task_ids       :List of 4 #>   .. .. .. .. .. ..$ origin_date:List of 2 #>   .. .. .. .. .. .. ..$ required: NULL #>   .. .. .. .. .. .. ..$ optional: chr [1:2] \"2022-10-01\" \"2022-10-08\" #>   .. .. .. .. .. ..$ target     :List of 2 #>   .. .. .. .. .. .. ..$ required: chr \"wk inc flu hosp\" #>   .. .. .. .. .. .. ..$ optional: NULL #>   .. .. .. .. .. ..$ horizon    :List of 2 #>   .. .. .. .. .. .. ..$ required: int 1 #>   .. .. .. .. .. .. ..$ optional: int [1:3] 2 3 4 #>   .. .. .. .. .. ..$ location   :List of 2 #>   .. .. .. .. .. .. ..$ required: NULL #>   .. .. .. .. .. .. ..$ optional: chr [1:54] \"US\" \"01\" \"02\" \"04\" ... #>   .. .. .. .. ..$ output_type    :List of 2 #>   .. .. .. .. .. ..$ mean    :List of 2 #>   .. .. .. .. .. .. ..$ output_type_id:List of 2 #>   .. .. .. .. .. .. .. ..$ required: NULL #>   .. .. .. .. .. .. .. ..$ optional: logi NA #>   .. .. .. .. .. .. ..$ value         :List of 2 #>   .. .. .. .. .. .. .. ..$ type   : chr \"integer\" #>   .. .. .. .. .. .. .. ..$ minimum: int 0 #>   .. .. .. .. .. ..$ quantile:List of 2 #>   .. .. .. .. .. .. ..$ output_type_id:List of 2 #>   .. .. .. .. .. .. .. ..$ required: num [1:23] 0.01 0.025 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 ... #>   .. .. .. .. .. .. .. ..$ optional: NULL #>   .. .. .. .. .. .. ..$ value         :List of 2 #>   .. .. .. .. .. .. .. ..$ type   : chr \"integer\" #>   .. .. .. .. .. .. .. ..$ minimum: int 0 #>   .. .. .. .. ..$ target_metadata:List of 1 #>   .. .. .. .. .. ..$ :List of 7 #>   .. .. .. .. .. .. ..$ target_id    : chr \"wk inc flu hosp\" #>   .. .. .. .. .. .. ..$ target_name  : chr \"Weekly incident influenza hospitalizations\" #>   .. .. .. .. .. .. ..$ target_units : chr \"count\" #>   .. .. .. .. .. .. ..$ target_keys  :List of 1 #>   .. .. .. .. .. .. .. ..$ target: chr \"wk inc flu hosp\" #>   .. .. .. .. .. .. ..$ target_type  : chr \"continuous\" #>   .. .. .. .. .. .. ..$ is_step_ahead: logi TRUE #>   .. .. .. .. .. .. ..$ time_unit    : chr \"week\" #>   .. .. ..$ submissions_due       :List of 3 #>   .. .. .. ..$ relative_to: chr \"origin_date\" #>   .. .. .. ..$ start      : int -6 #>   .. .. .. ..$ end        : int 1 #>   .. ..$ :List of 4 #>   .. .. ..$ round_id_from_variable: logi TRUE #>   .. .. ..$ round_id              : chr \"origin_date\" #>   .. .. ..$ model_tasks           :List of 1 #>   .. .. .. ..$ :List of 3 #>   .. .. .. .. ..$ task_ids       :List of 5 #>   .. .. .. .. .. ..$ origin_date:List of 2 #>   .. .. .. .. .. .. ..$ required: NULL #>   .. .. .. .. .. .. ..$ optional: chr [1:3] \"2022-10-15\" \"2022-10-22\" \"2022-10-29\" #>   .. .. .. .. .. ..$ target     :List of 2 #>   .. .. .. .. .. .. ..$ required: chr \"wk inc flu hosp\" #>   .. .. .. .. .. .. ..$ optional: NULL #>   .. .. .. .. .. ..$ horizon    :List of 2 #>   .. .. .. .. .. .. ..$ required: int 1 #>   .. .. .. .. .. .. ..$ optional: int [1:3] 2 3 4 #>   .. .. .. .. .. ..$ location   :List of 2 #>   .. .. .. .. .. .. ..$ required: NULL #>   .. .. .. .. .. .. ..$ optional: chr [1:54] \"US\" \"01\" \"02\" \"04\" ... #>   .. .. .. .. .. ..$ age_group  :List of 2 #>   .. .. .. .. .. .. ..$ required: chr \"65+\" #>   .. .. .. .. .. .. ..$ optional: chr [1:4] \"0-5\" \"6-18\" \"19-24\" \"25-64\" #>   .. .. .. .. ..$ output_type    :List of 2 #>   .. .. .. .. .. ..$ mean    :List of 2 #>   .. .. .. .. .. .. ..$ output_type_id:List of 2 #>   .. .. .. .. .. .. .. ..$ required: NULL #>   .. .. .. .. .. .. .. ..$ optional: logi NA #>   .. .. .. .. .. .. ..$ value         :List of 2 #>   .. .. .. .. .. .. .. ..$ type   : chr \"integer\" #>   .. .. .. .. .. .. .. ..$ minimum: int 0 #>   .. .. .. .. .. ..$ quantile:List of 2 #>   .. .. .. .. .. .. ..$ output_type_id:List of 2 #>   .. .. .. .. .. .. .. ..$ required: num [1:23] 0.01 0.025 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 ... #>   .. .. .. .. .. .. .. ..$ optional: NULL #>   .. .. .. .. .. .. ..$ value         :List of 2 #>   .. .. .. .. .. .. .. ..$ type   : chr \"integer\" #>   .. .. .. .. .. .. .. ..$ minimum: int 0 #>   .. .. .. .. ..$ target_metadata:List of 1 #>   .. .. .. .. .. ..$ :List of 7 #>   .. .. .. .. .. .. ..$ target_id    : chr \"wk inc flu hosp\" #>   .. .. .. .. .. .. ..$ target_name  : chr \"Weekly incident influenza hospitalizations\" #>   .. .. .. .. .. .. ..$ target_units : chr \"count\" #>   .. .. .. .. .. .. ..$ target_keys  :List of 1 #>   .. .. .. .. .. .. .. ..$ target: chr \"wk inc flu hosp\" #>   .. .. .. .. .. .. ..$ target_type  : chr \"continuous\" #>   .. .. .. .. .. .. ..$ is_step_ahead: logi TRUE #>   .. .. .. .. .. .. ..$ time_unit    : chr \"week\" #>   .. .. ..$ submissions_due       :List of 3 #>   .. .. .. ..$ relative_to: chr \"origin_date\" #>   .. .. .. ..$ start      : int -6 #>   .. .. .. ..$ end        : int 1 #>   ..- attr(*, \"schema_id\")= chr \"https://raw.githubusercontent.com/hubverse-org/schemas/main/v2.0.0/tasks-schema.json\" #>   ..- attr(*, \"type\")= chr \"tasks\" #>   ..- attr(*, \"class\")= chr [1:2] \"config\" \"list\" mod_out_path <- system.file(\"testhubs/simple/model-output\", package = \"hubUtils\") mod_out_con <- connect_model_output(mod_out_path) print(mod_out_con) #>  #> ── <mod_out_connection/FileSystemDataset> ── #>  #> • file_format: \"csv(3/3)\" #> • checks: TRUE #> • file_system: \"LocalFileSystem\" #> • model_output_dir: #>   \"/home/runner/work/_temp/Library/hubUtils/testhubs/simple/model-output\" #>  #> ── Connection schema  #> mod_out_connection with 3 csv files #> 8 columns #> origin_date: date32[day] #> target: string #> horizon: int64 #> location: string #> output_type: string #> output_type_id: double #> value: int64 #> model_id: string"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/r_to_arrow_datatypes.html","id":null,"dir":"Reference","previous_headings":"","what":"Create R type to Arrow DataType mapping — r_to_arrow_datatypes","title":"Create R type to Arrow DataType mapping — r_to_arrow_datatypes","text":"Returns named list mapping base R type strings (e.g., \"character\", \"integer\") corresponding Arrow arrow::DataType objects. inverse arrow_to_r_datatypes useful creating Arrow schemas programmatically R type specifications.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/r_to_arrow_datatypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create R type to Arrow DataType mapping — r_to_arrow_datatypes","text":"","code":"r_to_arrow_datatypes()"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/r_to_arrow_datatypes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create R type to Arrow DataType mapping — r_to_arrow_datatypes","text":"named list 6 entries mapping R types Arrow DataType objects: logical arrow::bool() integer arrow::int32() (uses int32 default) double arrow::float64() character arrow::utf8() Date arrow::date32() POSIXct arrow::timestamp(unit = \"ms\")","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/r_to_arrow_datatypes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create R type to Arrow DataType mapping — r_to_arrow_datatypes","text":"function generates mapping dynamically. R type strings match used non_task_id_schema field target-data.json configuration files. particularly useful : Creating custom Arrow schemas R type specifications Converting configuration-based type information Arrow schemas Programmatic schema generation","code":""},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/reference/r_to_arrow_datatypes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create R type to Arrow DataType mapping — r_to_arrow_datatypes","text":"","code":"# Get the mapping type_map <- r_to_arrow_datatypes()  # Use it to create Arrow types from R type strings r_types <- c(\"character\", \"integer\", \"double\") arrow_types <- type_map[r_types]  # Create a simple Arrow schema my_schema <- arrow::schema(   name = type_map[[\"character\"]],   age = type_map[[\"integer\"]],   score = type_map[[\"double\"]] ) my_schema #> Schema #> name: string #> age: int32 #> score: double"},{"path":"https://hubverse-org.github.io/hubData/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. hubUtils as_model_out_tbl, model_id_merge, model_id_split, validate_model_out_tbl","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/s3_bucket.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to an AWS S3 bucket — s3_bucket","title":"Connect to an AWS S3 bucket — s3_bucket","text":"See arrow::s3_bucket() details.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/s3_bucket.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to an AWS S3 bucket — s3_bucket","text":"SubTreeFileSystem containing S3FileSystem bucket's relative path. Note function's success guarantee authorized access bucket's contents.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/reference/s3_bucket.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to an AWS S3 bucket — s3_bucket","text":"","code":"if (FALSE) { bucket <- s3_bucket(\"voltrondata-labs-datasets\") } if (FALSE) { # Turn on debug logging. The following line of code should be run in a fresh # R session prior to any calls to `s3_bucket()` (or other S3 functions) Sys.setenv(\"ARROW_S3_LOG_LEVEL\" = \"DEBUG\") bucket <- s3_bucket(\"voltrondata-labs-datasets\") }"},{"path":[]},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"breaking-changes-development-version","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"hubData (development version)","text":"Use ignore_files argument exclude specific files, Set skip_checks = FALSE explicitly, Ensure model-output directories contain valid model output files Note: connect_model_output() retains default skip_checks = FALSE designed working model output directories may draft form.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"new-features-and-improvements-development-version","dir":"Changelog","previous_headings":"","what":"New features and improvements","title":"hubData (development version)","text":"Added comprehensive “Accessing Target Data” vignette demonstrating use connect_target_timeseries() connect_target_oracle_output() access target data, including filtering, joining model outputs, working cloud-based hubs (#108). Added r_to_arrow_datatypes() function providing inverse mapping R data types Arrow data types, enabling vectorized type conversion processing target-data.json configurations (#107). Enhanced create_timeseries_schema() create_oracle_output_schema() support config-based schema creation target-data.json (v6.0.0+) present (#107). enables fast, deterministic schema creation without filesystem /O, especially beneficial cloud storage. Functions automatically fall back inference-based schema creation pre-v6 hubs hubs without target-data.json, maintaining backward compatibility. functionality propagated connect_target_timeseries() connect_target_oracle_output(), use schema creation functions internally. Enhanced documentation connect_target_timeseries() connect_target_oracle_output() clarify column ordering behavior: v6+ Parquet files reordered hubverse convention, CSV files preserve original ordering avoid column name/position mismatches collection (#107). Added get_target_data_colnames() function extracting ordering expected column names target data target-data.json configuration files (#109).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-150","dir":"Changelog","previous_headings":"","what":"hubData 1.5.0","title":"hubData 1.5.0","text":"as_r_schema(): Converts Arrow schema named character vector equivalent R types (e.g., \"int32\" → \"integer\"). Errors unsupported types. arrow_schema_to_string(): Extracts raw Arrow type strings field schema. is_supported_arrow_type(): Returns named logical vector indicating schema fields supported types. validate_arrow_schema(): Validates field types Arrow schema supported. Throws helpful error otherwise. Added arrow_to_r_datatypes, named character vector defining mapping safe portable Arrow types R equivalents. Added r_schema argument create_timeseries_schema() create_oracle_output_schema() functions enable returning schema vector R data types instead arrow::Schema object (#95) Added output_type_id_datatype argument create_oracle_output_schema() connect_target_oracle_output() functions allow users explicitly specify data type output_type_id column schema. ensuring compatibility create_hub_schema() connect_hub() (#95). (Internal) Refactored target data schema connection tests use embedded example hubs reusable schema fixtures, improving reliability making tests independent dataset size ordering. extract_hive_partitions() extracting key value pairs paths hive-partitioned data files. is_hive_partitioned_path() checking path hive-partitioned. create_oracle_output_schema() create_timeseries_schema() now define schema hive-partitions whose data types defined tasks.json config (#89).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-140","dir":"Changelog","previous_headings":"","what":"hubData 1.4.0","title":"hubData 1.4.0","text":"Added connect_target_timeseries() function (experimental) accessing time-series target data hub (#71). includes accessing target data cloud hubs (#75). Added create_timeseries_schema() function creating schema time-series target data (#71). Added connect_target_oracle_output() function (experimental) accessing oracle-output target data hub (#72). includes accessing target data cloud hubs (#76). Added create_oracle_output_schema() function creating schema oracle-output target data (#72). Added get_target_path() function retrieving path appropriate target data file directory hub. Added get_target_file_ext() function retrieving file extensions target data file(s) hub. Added get_s3_bucket_name() extracting bucket name cloud enabled hub hub’s config (#75). Added na argument connect_hub(), connect_model_output(), connect_target_timeseries(), connect_target_oracle_output(), create_timeseries_schema(), create_oracle_output_schema() allow specification handle missing values CSV files. default use NA \"\", users can restrict \"\" (empty string) needing include character \"NA\" values CSV data (#80). Note approach works NA values written CSV file \"\" (empty string) NA \"NA\". Added ignore_files argument connect_hub() connect_model_output() allow users specify vector file name prefixes ignore scanning hub’s model output directory files. useful excluding files relevant hub’s model output, README files documentation well potentially invalid files (#87). feature also used internally connect_hub() enable skipping expensive file validity checks connecting cloud-based hubs multiple file formats using skip_checks = TRUE. Refactored connect_hub() connect_model_output() internally reduce number calls cloud hubs, improving performance connecting cloud-based hubs. Added ignore_files argument connect_target_oracle_output(), connect_target_timeseries(), create_timeseries_schema(), create_oracle_output_schema() allow users specify vector file name prefixes ignore scanning hub’s target data directory files (#87).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-130","dir":"Changelog","previous_headings":"","what":"hubData 1.3.0","title":"hubData 1.3.0","text":"Support determination hub schema v4 configuration files (#63). Also fixes bug create_hub_schema() output_type_id data type incorrectly auto-determined logical point estimate output types collected hub. Now character data type returned output_type_id schema versions situations auto-determined.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-123","dir":"Changelog","previous_headings":"","what":"hubData 1.2.3","title":"hubData 1.2.3","text":"Fix bug create_hub_schema() output_type_id data type incorrectly determined Date instead character (Reported https://github.com/reichlab/variant-nowcast-hub/pull/87#issuecomment-2387372238).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-122","dir":"Changelog","previous_headings":"","what":"hubData 1.2.2","title":"hubData 1.2.2","text":"Remove dependency development version arrow package bump required version 17.0.0.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-121","dir":"Changelog","previous_headings":"","what":"hubData 1.2.1","title":"hubData 1.2.1","text":"Removed dependency development version zoltr package. Fixed minor error connect_hub() article.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-120","dir":"Changelog","previous_headings":"","what":"hubData 1.2.0","title":"hubData 1.2.0","text":"model output directory contains model output data (README.md, example) model output files use single file format.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-111","dir":"Changelog","previous_headings":"","what":"hubData 1.1.1","title":"hubData 1.1.1","text":"Fix {tidyselect} warnings converting internal syntax Bump required dplyr version 1.1.0","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-110","dir":"Changelog","previous_headings":"","what":"hubData 1.1.0","title":"hubData 1.1.0","text":"Add \"from_config\" option output_type_id_datatype argument create_hub_schema(), coerce_to_hub_schema() connect_hub(). allows users set hub level output_type_id column data type tasks.json output_type_id_datatype property introduced schema version v3.0.1. (#44)","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-100","dir":"Changelog","previous_headings":"","what":"hubData 1.0.0","title":"hubData 1.0.0","text":"Breaking change: expand_model_out_val_grid() create_model_out_submit_tmpl() now defunct. functions moved hubValidations replaced hubValidations::expand_model_out_grid() hubValidations::submission_tmpl(), respectively. old functions now fail called removed future release.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-020","dir":"Changelog","previous_headings":"","what":"hubData 0.2.0","title":"hubData 0.2.0","text":"Adds back-compatible support create_hub_schema() determining hub’s schema v3.0.0 sample output type configurations tasks.json files (#27). Adds back-compatible support v3.0.0 sample output type configuration tasks.json files. primary change output_type_id values sample output types handled expand_model_out_val_grid(). default, valid task ID value combinations expanded, per output type, NAs returned output_type_id column. However, new argument include_sample_ids set TRUE, example sample IDs included output_type_id column, demonstrating compound tasks IDs group rows task ID combinations samples. unique across modeling tasks. create_model_out_submit_tmpl(), example sample IDs included output_type_id column default (#30).","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-011","dir":"Changelog","previous_headings":"","what":"hubData 0.1.1","title":"hubData 0.1.1","text":"Add collect_zoltar(), retrieves data zoltardata.com project transforms Zoltar’s native download format hubverse one. Zoltar (documentation ) pre-hubverse research project implements repository model forecast results, including tools administer, query, visualize uploaded data, along R Python APIs access data programmatically (zoltr zoltpy, respectively.)","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-010","dir":"Changelog","previous_headings":"","what":"hubData 0.1.0","title":"hubData 0.1.0","text":"Add collect_hub() wraps dplyr::collect() , possible, converts output model_out_tbl class object default. function also accepts additional arguments can passed as_model_out_tbl(). Allow parsing tasks.json config files required optional properties task IDs set null. change facilitates encoding task IDs modeling tasks value expected given task ID. model output files, value modeling task task IDs NA.","code":""},{"path":"https://hubverse-org.github.io/hubData/dev/news/index.html","id":"hubdata-001","dir":"Changelog","previous_headings":"","what":"hubData 0.0.1","title":"hubData 0.0.1","text":"Initial package release resulting split hubUtils package. See hubUtils NEWS.md details including previous release notes.","code":""}]
